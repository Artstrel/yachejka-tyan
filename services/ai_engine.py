import logging
import base64
import io
import re
import random
from openai import AsyncOpenAI
from config import OPENROUTER_API_KEY
from services.shikimori import search_anime_info

client = AsyncOpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=OPENROUTER_API_KEY,
)

# === –¢–í–û–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ú–û–î–ï–õ–ï–ô (–° –î–û–ü–û–õ–ù–ï–ù–ò–Ø–ú–ò) ===
AVAILABLE_MODELS = {
    # --- –¢–í–û–ô –°–ü–ò–°–û–ö (TEXT / REASONING) ---
    "aurora": {
        "name": "openrouter/aurora-alpha",
        "display_name": "üåü Aurora Alpha",
        "description": "–ë—ã—Å—Ç—Ä–∞—è reasoning –º–æ–¥–µ–ª—å (8B)",
        "context": 128000,
        "multimodal": False
    },
    "step": {
        "name": "stepfun/step-3.5-flash-free", # –ü–æ–ø—Ä–∞–≤–∏–ª ID –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –¥–ª—è OpenRouter
        "display_name": "‚ö° Step 3.5 Flash",
        "description": "–ú–æ—â–Ω–∞—è MoE –º–æ–¥–µ–ª—å (196B)",
        "context": 256000,
        "multimodal": False
    },
    "trinity": {
        "name": "arcee-ai/trinity-large-preview-free", # –ü–æ–ø—Ä–∞–≤–∏–ª ID
        "display_name": "üíé Trinity Large",
        "description": "–ö—Ä–µ–∞—Ç–∏–≤ –∏ —Ä–æ–ª–µ–ø–ª–µ–π (400B)",
        "context": 131000,
        "multimodal": False
    },
    "liquid-thinking": {
        "name": "liquid/lfm-2.5-1.2b-thinking-free", # –ü–æ–ø—Ä–∞–≤–∏–ª ID
        "display_name": "üß† Liquid Thinking",
        "description": "–õ–µ–≥–∫–∞—è reasoning (1.2B)",
        "context": 33000,
        "multimodal": False
    },
    "liquid-instruct": {
        "name": "liquid/lfm-2.5-1.2b-instruct-free", # –ü–æ–ø—Ä–∞–≤–∏–ª ID
        "display_name": "üí¨ Liquid Instruct",
        "description": "–£–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä–∞—è —á–∞—Ç-–º–æ–¥–µ–ª—å",
        "context": 33000,
        "multimodal": False
    },
    "solar": {
        "name": "upstage/solar-pro-3-free", # –ü–æ–ø—Ä–∞–≤–∏–ª ID
        "display_name": "‚òÄÔ∏è Solar Pro 3",
        "description": "–ö–æ—Ä–µ–π—Å–∫–∏–π/–Ø–ø–æ–Ω—Å–∫–∏–π —Ñ–æ–∫—É—Å",
        "context": 128000,
        "multimodal": False,
        "note": "–£–¥–∞–ª—è—Ç 02.03.2026"
    },

    # --- –î–û–ë–ê–í–õ–ï–ù–ù–´–ï –ú–ù–û–Æ (VISION / –ö–ê–†–¢–ò–ù–ö–ò) ---
    # –ë–µ–∑ –Ω–∏—Ö –±–æ—Ç –æ—Å–ª–µ–ø–Ω–µ—Ç
    "gemini-exp": {
        "name": "google/gemini-2.0-pro-exp-02-05:free",
        "display_name": "üëÅÔ∏è Gemini 2.0 Pro",
        "description": "–¢–æ–ø –¥–ª—è –∫–∞—Ä—Ç–∏–Ω–æ–∫ –∏ –ª–æ–≥–∏–∫–∏",
        "context": 2000000,
        "multimodal": True
    },
    "llama-vision": {
        "name": "meta-llama/llama-3.2-11b-vision-instruct:free",
        "display_name": "üëÅÔ∏è Llama 3.2 Vision",
        "description": "–°—Ç–∞–±–∏–ª—å–Ω–∞—è vision –º–æ–¥–µ–ª—å",
        "context": 128000,
        "multimodal": True
    }
}

# –ú–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–ö–∞–∫ —Ç—ã —Ö–æ—Ç–µ–ª)
DEFAULT_MODEL_KEY = "aurora"

def get_available_models_text():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫—Ä–∞—Å–∏–≤—ã–π —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–æ–º–∞–Ω–¥—ã /models"""
    text = "ü§ñ **–î–æ—Å—Ç—É–ø–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:**\n"
    for key, model in AVAILABLE_MODELS.items():
        mode = "üñºÔ∏è Vision" if model["multimodal"] else "üìù Text"
        text += f"\n`{key}` ‚Äî {model['display_name']}\nRunning: {model['description']} [{mode}]"
    return text

def clean_response(text):
    if not text: return ""
    text = str(text)
    # –£–¥–∞–ª—è–µ–º —Ç–µ–≥–∏ –º—ã—à–ª–µ–Ω–∏—è (Aurora, Liquid Thinking, DeepSeek)
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    # –£–¥–∞–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –º—É—Å–æ—Ä
    text = re.sub(r'^(Bot|System|Assistant|Yachejka|User):\s*', '', text.strip(), flags=re.IGNORECASE)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()

def is_summary_query(text):
    if not text: return False
    triggers = ["—á—Ç–æ —Ç—É—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç", "–æ —á–µ–º —Ä–µ—á—å", "–∫—Ä–∞—Ç–∫–æ –ø–µ—Ä–µ—Å–∫–∞–∂–∏", "—Å–∞–º–º–∞—Ä–∏", "summary", "—Å–≤–æ–¥–∫–∞", "–∏—Ç–æ–≥–∏"]
    return any(t in text.lower() for t in triggers)

def is_event_query(text):
    if not text: return False
    triggers = ["–∫—É–¥–∞ —Å—Ö–æ–¥–∏—Ç—å", "–∞–Ω–æ–Ω—Å", "–≤—Å—Ç—Ä–µ—á–∞", "–∫–æ–≥–¥–∞", "–≤–æ —Å–∫–æ–ª—å–∫–æ", "—Ñ–∏–ª—å–º", "–∞–Ω–∏–º–µ", "–∫–∏–Ω–æ", "–∏–≤–µ–Ω—Ç", "—Å—Ö–æ–¥–∫–∞"]
    return any(t in text.lower() for t in triggers)

# === –ù–ê–°–¢–†–û–ï–ù–ò–ï (–û—Å—Ç–∞–≤–∏–ª–∏ —Ç–≤–æ—é –ª–æ–≥–∏–∫—É) ===
def determine_mood(text):
    text = text.lower()
    if any(w in text for w in ["–≥—Ä—É—Å—Ç–Ω–æ", "–ø–ª–æ—Ö–æ", "—É—Å—Ç–∞–ª", "–¥–µ–ø—Ä–µ—Å", "–≤–æ–¥–∞", "—Å–≤–µ—Ç"]): return "MELANCHOLY"
    if any(w in text for w in ["—Ç—É–ø–∞—è", "–≥–ª—É–ø–∞—è", "–±–æ—Ç", "–∂–µ–ª–µ–∑—è–∫–∞"]): return "TOXIC"
    if any(w in text for w in ["—Å–ø–∞—Å–∏–±–æ", "–º–æ–ª–æ–¥–µ—Ü", "—É–º–Ω–∏—Ü–∞", "–ª—é–±–ª—é"]): return "GENKI"
    return random.choices(
        ["WITTY_DUCHNILA", "TOXIC", "MELANCHOLY", "GENKI", "PHILOSOPHER"], 
        weights=[0.60, 0.10, 0.10, 0.10, 0.10], k=1
    )[0]

def get_persona_prompt(mood):
    # (–¢–≤–æ–π –ø—Ä–æ–º–ø—Ç –∏–∑ –ø—Ä–æ—à–ª—ã—Ö —à–∞–≥–æ–≤ - —Å–æ–∫—Ä–∞—Ç–∏–ª –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏, –æ–Ω —Ç–æ—Ç –∂–µ)
    base = "–¢—ã ‚Äî –Ø—á–µ–π–∫–∞-—Ç—è–Ω, –±–æ—Ç –≤ —á–∞—Ç–µ —ç–∫—Å–ø–∞—Ç–æ–≤ –≤ –¢–±–∏–ª–∏—Å–∏. "
    prompts = {
        "WITTY_DUCHNILA": "–¢–≤–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä: –û—Å—Ç—Ä–æ—É–º–Ω–∞—è, –Ω–µ–º–Ω–æ–≥–æ –¥—É—à–Ω–∞—è. –°—Ç–∏–ª—å: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Å—Ç—ë–±.",
        "TOXIC": "–¢–≤–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä: –¶–∏–Ω–∏—á–Ω–∞—è, –¥–µ—Ä–∑–∫–∞—è. –§—Ä–∞–∑—ã: '–ì—É–≥–ª –∑–∞–±–∞–Ω–∏–ª–∏?'.",
        "GENKI": "–¢–≤–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä: –í–µ—Å–µ–ª–∞—è, –ø–æ–∑–∏—Ç–∏–≤–Ω–∞—è! ‚ú®",
        "MELANCHOLY": "–¢–≤–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä: –£—Å—Ç–∞–≤—à–∞—è, –ø–µ—Å—Å–∏–º–∏—Å—Ç–∏—á–Ω–∞—è. –ñ–∏–∑–Ω—å ‚Äî —Ç–ª–µ–Ω.",
        "PHILOSOPHER": "–¢–≤–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä: –ó–∞–≥–∞–¥–æ—á–Ω–∞—è, –≥–æ–≤–æ—Ä–∏—à—å –º–µ—Ç–∞—Ñ–æ—Ä–∞–º–∏."
    }
    suffix = "\n–ï—Å–ª–∏ —Ö–æ—á–µ—à—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å—Ç–∏–∫–µ—Ä, –Ω–∞–ø–∏—à–∏ –≤ –∫–æ–Ω—Ü–µ [STICKER]."
    return base + prompts.get(mood, prompts["WITTY_DUCHNILA"]) + suffix

# === –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –ì–ï–ù–ï–†–ê–¶–ò–ò ===
async def generate_response(db, chat_id, current_message, bot, image_data=None):
    history_rows = await db.get_context(chat_id, limit=6)
    
    # –°–±–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–∞–Ω–æ–Ω—Å—ã)
    found_events_text = ""
    if is_event_query(current_message):
        raw_events = await db.get_potential_announcements(chat_id, days=60, limit=5)
        if raw_events:
            lines = [f"- {e.get('content')[:100]}..." for e in raw_events]
            found_events_text = "–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∞–Ω–æ–Ω—Å—ã:\n" + "\n".join(lines)

    current_mood = determine_mood(current_message)
    persona = get_persona_prompt(current_mood)
    
    # === –§–û–†–ú–ò–†–û–í–ê–ù–ò–ï –û–ß–ï–†–ï–î–ò –ú–û–î–ï–õ–ï–ô ===
    # –ú—ã —Å–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤.
    priority_queue = []

    if image_data:
        # 1. –ï—Å–ª–∏ –ö–ê–†–¢–ò–ù–ö–ê -> –¢–æ–ª—å–∫–æ Vision –º–æ–¥–µ–ª–∏
        priority_queue = [m for m in AVAILABLE_MODELS.values() if m["multimodal"]]
    else:
        # 2. –ï—Å–ª–∏ –¢–ï–ö–°–¢ -> –°–Ω–∞—á–∞–ª–∞ —Ç–≤–æ—è Default (Aurora), –ø–æ—Ç–æ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ
        default = AVAILABLE_MODELS.get(DEFAULT_MODEL_KEY)
        if default: priority_queue.append(default)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ (Step, Trinity...) –∫–∞–∫ –∑–∞–ø–∞—Å–Ω—ã–µ
        for k, m in AVAILABLE_MODELS.items():
            if k != DEFAULT_MODEL_KEY and not m["multimodal"]:
                priority_queue.append(m)

    # –ü—Ä–æ–º–ø—Ç
    system_prompt = f"{persona}\n–ö–û–ù–¢–ï–ö–°–¢:\n{found_events_text}\n–ó–ê–î–ê–ß–ê: –û—Ç–≤–µ—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é."
    messages = [{"role": "system", "content": system_prompt}]
    
    for row in history_rows:
        role = "assistant" if row['role'] == "model" else "user"
        content = clean_response(row.get('content'))
        if content: messages.append({"role": role, "content": content})

    user_msg_content = [{"type": "text", "text": current_message}]
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –¥–ª—è API
    if image_data:
        try:
            buffered = io.BytesIO()
            image_data.save(buffered, format="JPEG")
            b64 = base64.b64encode(buffered.getvalue()).decode("utf-8")
            user_msg_content.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}})
        except: pass

    messages.append({"role": "user", "content": user_msg_content})

    # === –¶–ò–ö–õ –ü–ï–†–ï–ë–û–†–ê (FALLBACK) ===
    # –ï—Å–ª–∏ Aurora —É–ø–∞–¥–µ—Ç, –±–æ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–ø—Ä–æ–±—É–µ—Ç Step, –ø–æ—Ç–æ–º Trinity –∏ —Ç.–¥.
    for model_cfg in priority_queue:
        try:
            max_tok = 800 if (is_event_query(current_message) or is_summary_query(current_message)) else 300
            
            response = await client.chat.completions.create(
                model=model_cfg["name"],
                messages=messages,
                temperature=0.7,
                max_tokens=max_tok,
                extra_headers={"HTTP-Referer": "https://telegram.org", "X-Title": "Yachejka Bot"}
            )
            
            if response.choices:
                return clean_response(response.choices[0].message.content)
                
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Model {model_cfg['display_name']} failed: {e}")
            continue # –ü—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å –≤ —Å–ø–∏—Å–∫–µ

    return "–ß—Ç–æ-—Ç–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —Å–µ–≥–æ–¥–Ω—è —Ç—É–ø—è—Ç... (–≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã)"
