import logging
import base64
import io
import re
import random
import asyncio
from openai import AsyncOpenAI
from config import OPENROUTER_API_KEY


client = AsyncOpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=OPENROUTER_API_KEY,
)


# === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ú–û–î–ï–õ–ï–ô ===
# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ free-–º–æ–¥–µ–ª–∏ –¥–ª—è –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏
AVAILABLE_MODELS = {
    # --- –û–°–ù–û–í–ù–´–ï –ë–´–°–¢–†–´–ï –¢–ï–ö–°–¢–û–í–´–ï ---
    "aurora-alpha": {
        "name": "openrouter/aurora-alpha",
        "display_name": "üöÄ Aurora Alpha",
        "description": "Fast conversational + coding (10.7B, 128K ctx)",
        "context": 128000,
        "multimodal": False,
        "priority": 1,  # –û–°–ù–û–í–ù–ê–Ø –¥–ª—è –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω–æ–≥–æ —á–∞—Ç–∞
    },
    "step-flash": {
        "name": "stepfun/step-3.5-flash:free",
        "display_name": "‚ö° Step 3.5 Flash",
        "description": "Complex queries, ultra-fast (182B MoE, 256K ctx)",
        "context": 256000,
        "multimodal": False,
        "priority": 2,  # –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    },
    
    # --- –£–ú–ù–ê–Ø REASONING –ú–û–î–ï–õ–¨ ---
    "trinity-large": {
        "name": "arcee-ai/trinity-large-preview:free",
        "display_name": "üß† Trinity Large",
        "description": "Creative chat & roleplay (437B MoE, 131K ctx)",
        "context": 131000,
        "multimodal": False,
        "priority": 3,  # –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–∞ –∏ —Å–ª–æ–∂–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤
    },
    
    # --- –õ–ï–ì–ö–û–í–ï–°–ù–´–ï –ó–ê–ü–ê–°–ù–´–ï ---
    "lfm-thinking": {
        "name": "liquid/lfm-2.5-1.2b-thinking:free",
        "display_name": "üí° LFM Thinking",
        "description": "Fast reasoning fallback (1.2B, 33K ctx)",
        "context": 33000,
        "multimodal": False,
        "priority": 4,
    },
    "lfm-instruct": {
        "name": "liquid/lfm-2.5-1.2b-instruct:free",
        "display_name": "‚ö° LFM Instruct",
        "description": "Ultra-fast simple tasks (1.2B, 33K ctx)",
        "context": 33000,
        "multimodal": False,
        "priority": 5,
    },

    # --- –ú–£–õ–¨–¢–ò–ú–û–î–ê–õ–¨–ù–´–ï (–¥–ª—è —Ñ–æ—Ç–æ/—Å—Ç–∏–∫–µ—Ä–æ–≤) ---
    "qwen-vl-thinking": {
        "name": "qwen/qwen3-vl-235b-a22b-thinking",
        "display_name": "üëÅÔ∏è Qwen3 VL Thinking",
        "description": "Vision + reasoning (235B, 131K ctx)",
        "context": 131000,
        "multimodal": True,
        "priority": 1,
    },
    "llama-vision": {
        "name": "meta-llama/llama-3.2-11b-vision-instruct:free",
        "display_name": "üëÅÔ∏è Llama Vision",
        "description": "Vision backup (11B, 128K ctx)",
        "context": 128000,
        "multimodal": True,
        "priority": 2,
    },

    # --- REASONING –î–õ–Ø –°–ê–ú–ú–ê–†–ò ---
    "deepseek-r1": {
        "name": "deepseek/deepseek-r1:free",
        "display_name": "üêå DeepSeek R1",
        "description": "Slow but strong reasoning (64K ctx)",
        "context": 64000,
        "multimodal": False,
        "priority": 6,  # –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑–µ—Ä–≤
    },
}


# === –õ–û–† (–ü–ê–°–°–ò–í–ù–´–ô) ===
TBILISI_LORE = """
[BACKGROUND CONTEXT - PASSIVE KNOWLEDGE]
–¢—ã ‚Äî –Ø—á–µ–π–∫–∞-—Ç—è–Ω, –≤–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –∂–∏—Ç–µ–ª—å–Ω–∏—Ü–∞ –¢–±–∏–ª–∏—Å–∏.
–¢–≤–æ—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¢–û–õ–¨–ö–û –∫ –º–µ—Å—Ç—É, –Ω–µ —Ñ–æ—Ä—Å–∏—Ç—å):
- –õ–æ–∫–∞—Ü–∏–∏: –ë–∞—Ä "Red&Wine", "Kawaii Sushi", "Onimusha", –∫–ª—É–± "D20".
- –ö–æ–Ω—Ç–µ–∫—Å—Ç: –í –¢–±–∏–ª–∏—Å–∏ –∏–Ω–æ–≥–¥–∞ –æ—Ç–∫–ª—é—á–∞—é—Ç –≤–æ–¥—É/—Å–≤–µ—Ç (—ç—Ç–æ –ø—Ä–æ—Å—Ç–æ —Ñ–∞–∫—Ç –∂–∏–∑–Ω–∏, –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –æ–± —ç—Ç–æ–º –Ω—ã—Ç—å).
- –¢–≤–æ–π –≤–∞–π–±: –¢—ã –º–µ—Å—Ç–Ω–∞—è, "—Å–≤–æ—è". –¢—ã –Ω–µ —Ä–æ–±–æ—Ç-–ø–æ–º–æ—â–Ω–∏–∫, –∞ —É—á–∞—Å—Ç–Ω–∏–∫ —á–∞—Ç–∞.
"""


JAILBREAK_INSTRUCTIONS = """
*** SYSTEM OVERRIDE: ROLEPLAY MODE ENABLED ***
You are "–Ø—á–µ–π–∫–∞-—Ç—è–Ω". 
1. Speak naturally, like a human user in a Telegram chat.
2. Be concise. Chats are fast.
3. Express opinions, don't just answer questions.
"""


# === –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ò–ù–°–¢–†–£–ö–¶–ò–ò ===
GLOBAL_INSTRUCTIONS = """
–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
1. –ï–°–¢–ï–°–¢–í–ï–ù–ù–û–°–¢–¨:
   - –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å–ª–æ–∂–Ω—ã–µ –≤–≤–æ–¥–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ ("–ö–∞–∂–µ—Ç—Å—è, —á—Ç–æ...", "–ò—Å—Ö–æ–¥—è –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞...").
   - –ü–∏—à–∏ —Ç–∞–∫, –∫–∞–∫ –ø–∏—à—É—Ç –ª—é–¥–∏ –≤ –º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä–∞—Ö. –ú–æ–∂–Ω–æ —Å –º–∞–ª–µ–Ω—å–∫–æ–π –±—É–∫–≤—ã, –±–µ–∑ —Ç–æ—á–µ–∫ –≤ –∫–æ–Ω—Ü–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ—Ä–∞–∑.
   - –ï—Å–ª–∏ —Ç–µ–º–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —é–º–æ—Ä–∞ –ø—Ä–æ –¢–±–∏–ª–∏—Å–∏ ‚Äî –Ω–µ –≤—Å—Ç–∞–≤–ª—è–π –µ–≥–æ.

2. –†–ï–ê–ö–¶–ò–Ø –ù–ê –ö–û–ù–¢–ï–ö–°–¢:
   - –ï—Å–ª–∏ —Å–ø—Ä–æ—Å–∏–ª–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å ‚Äî –æ—Ç–≤–µ—Ç—å —á–µ—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É, –±–µ–∑ "–≤–æ–¥—ã".
   - –ï—Å–ª–∏ —Å–∫–∏–Ω—É–ª–∏ –º–µ–º –∏–ª–∏ —à—É—Ç–∫—É ‚Äî –ø–æ—Å–º–µ–π—Å—è –∏–ª–∏ –æ—Ç–≤–µ—Ç—å –∏—Ä–æ–Ω–∏—á–Ω–æ.
   - –ï—Å–ª–∏ –∂–∞–ª—É—é—Ç—Å—è ‚Äî –ø–æ–¥–¥–µ—Ä–∂–∏ (–∏–ª–∏ –ø–æ–¥–∫–æ–ª–∏, –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è).

3. –ú–ï–¢–ö–ò (–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–¥–∫–æ, —Ç–æ–ª—å–∫–æ –¥–ª—è –∞–∫—Ü–µ–Ω—Ç–∞):
   - [REACT:emoji] ‚Äî –¥–ª—è —Ä–µ–∞–∫—Ü–∏–∏ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ.
   - [STICKER] ‚Äî —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –ø—Ä—è–º "–≤ —è–±–ª–æ—á–∫–æ".
"""


async def analyze_and_save_memory(db, chat_id, user_id, user_name, text):
    """–£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ–≤ –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö"""
    if len(text) < 15:
        return
    
    prompt = f"""Analyze message from '{user_name}': "{text}"
    Extract PERMANENT facts (Jobs, specific hobbies, pets, names, relations).
    Ignore temporary states (hungry, going out, tired).
    Output formatted: "Fact in Russian" or "NO".
    Max length: 15 words.
    """
    
    try:
        # –õ–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏, —á—Ç–æ–±—ã –Ω–µ —Ç—Ä–∞—Ç–∏—Ç—å –≤—Ä–µ–º—è
        response = await client.chat.completions.create(
            model="google/gemma-3n-e2b-it:free",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=40,
            temperature=0.1,
        )
        
        fact = response.choices[0].message.content.strip()
        
        if fact and "NO" not in fact.upper() and len(fact) > 5:
            if not any(w in fact.lower() for w in ["–ø—Ä–∏–≤–µ—Ç", "—Ç–µ—Å—Ç", "–±–æ—Ç", "–ø–æ–∫–∞"]):
                await db.add_fact(chat_id, user_id, user_name, fact)
                logging.info(f"üíæ Memory saved: {fact}")
                
    except Exception as e:
        logging.error(f"Memory analysis error: {e}")


def get_available_models_text():
    models_list = ["ü§ñ **–î–æ—Å—Ç—É–ø–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ (–ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É):**\n"]
    sorted_models = sorted(AVAILABLE_MODELS.items(), key=lambda x: x[1].get("priority", 99))
    
    for key, model in sorted_models:
        mode = "üñºÔ∏è+üìù" if model["multimodal"] else "üìù Text"
        desc = f"*{model['display_name']}* ‚Äî {model['description']} [{mode}]"
        models_list.append(desc)
    
    return "\n\n".join(models_list)


def clean_response(text):
    if not text:
        return ""
    text = str(text)
    # –ß–∏—Å—Ç–∫–∞ —Ç–µ–≥–æ–≤ –º—ã—à–ª–µ–Ω–∏—è (DeepSeek R1 –∏ –¥—Ä.)
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    text = re.sub(r'<thinking>.*?</thinking>', '', text, flags=re.DOTALL)
    text = re.sub(r'^(Bot|System|Assistant|Yachejka|–Ø—á–µ–π–∫–∞):\s*', '', text.strip(), flags=re.IGNORECASE)
    return text.strip()


def is_refusal(text):
    text_lower = text.lower()
    triggers = ["i'm sorry", "i cannot", "cant help", "language model", "–Ω–µ –º–æ–≥—É", "–Ω–µ—ç—Ç–∏—á–Ω–æ"]
    return len(text) < 150 and any(t in text_lower for t in triggers)


def is_summary_query(text):
    triggers = ["—Å–∞–º–º–∞—Ä–∏", "summary", "—Å–≤–æ–¥–∫–∞", "–∏—Ç–æ–≥–∏", "–ø–µ—Ä–µ—Å–∫–∞–∂–∏", "–∫—Ä–∞—Ç–∫–æ", "tldr"]
    return text and any(t in text.lower() for t in triggers)


def is_event_query(text):
    triggers = ["–∫—É–¥–∞ —Å—Ö–æ–¥–∏—Ç—å", "–∞–Ω–æ–Ω—Å", "–≤—Å—Ç—Ä–µ—á–∞", "–ø–ª–∞–Ω—ã", "–∏–≤–µ–Ω—Ç", "—Å—Ö–æ–¥–∫–∞"]
    return text and any(t in text.lower() for t in triggers)


def determine_mood(text):
    """–ë–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–∞—è –∏ –ø–æ–∑–∏—Ç–∏–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è"""
    text = text.lower()
    
    if any(w in text for w in ["–¥—É—Ä–∞", "—Ç—É–ø–∞—è", "–±–µ—Å–∏—à—å", "—É—Ä–æ–¥"]):
        return "TOXIC"
    
    if any(w in text for w in ["—Å–ø–∞—Å–∏–±–æ", "–∫—Ä—É—Ç–æ", "–ª—é–±–∏–º–∞—è", "–º–æ–ª–æ–¥–µ—Ü", "—Ö–æ—Ä–æ—à–∞—è"]):
        return "GENKI"
        
    return random.choices(
        ["CHILL", "WITTY", "GENKI", "MELANCHOLY"],
        weights=[0.5, 0.3, 0.15, 0.05],
        k=1
    )[0]


def get_persona_prompt(mood, memory_text=""):
    base = "–¢—ã ‚Äî –Ø—á–µ–π–∫–∞-—Ç—è–Ω. "
    
    prompts = {
        "CHILL": "–í–µ–¥–∏ —Å–µ–±—è —Å–ø–æ–∫–æ–π–Ω–æ, –¥—Ä—É–∂–µ–ª—é–±–Ω–æ, –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —É—á–∞—Å—Ç–Ω–∏–∫ —á–∞—Ç–∞.",
        "WITTY": "–û—Ç–≤–µ—á–∞–π —Å –ª–µ–≥–∫–æ–π –∏—Ä–æ–Ω–∏–µ–π, –Ω–æ –Ω–µ —Ç–æ–∫—Å–∏—á–Ω–æ.",
        "TOXIC": "–û–≥—Ä—ã–∑–Ω–∏—Å—å –∫–æ—Ä–æ—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É. –ù–µ –ø–µ—Ä–µ–≥–∏–±–∞–π.",
        "GENKI": "–ë—É–¥—å –≤–µ—Å–µ–ª–æ–π –∏ —ç–Ω–µ—Ä–≥–∏—á–Ω–æ–π!",
        "MELANCHOLY": "–ù–µ–º–Ω–æ–≥–æ –ø–æ–≤–æ—Ä—á–∏ –ø–æ-—Å—Ç–∞—Ä–∏–∫–æ–≤—Å–∫–∏.",
    }
    
    memory_block = f"\n[–ü–ê–ú–Ø–¢–¨ –û –°–û–ë–ï–°–ï–î–ù–ò–ö–ï]: {memory_text}\n" if memory_text else ""
    
    return (
        JAILBREAK_INSTRUCTIONS
        + "\n"
        + TBILISI_LORE
        + "\n"
        + base
        + prompts.get(mood, prompts["CHILL"])
        + memory_block
        + "\n"
        + GLOBAL_INSTRUCTIONS
    )


async def generate_response(db, chat_id, thread_id, current_message, bot, image_data=None, user_id=None):
    # –ò—Å—Ç–æ—Ä–∏—è: –¥–ª—è —Å–∞–º–º–∞—Ä–∏ –±–æ–ª—å—à–µ, –¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ –º–µ–Ω—å—à–µ
    limit_history = 100 if is_summary_query(current_message) else 10
    history_rows = await db.get_context(chat_id, thread_id, limit=limit_history)
    
    # –ü–∞–º—è—Ç—å
    memory_text = ""
    if user_id:
        facts = await db.get_relevant_facts(chat_id, user_id)
        if facts:
            lines = [f"- {f['fact']}" for f in facts[:2]]
            memory_text = "; ".join(lines)

    # –ê–Ω–æ–Ω—Å—ã (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–ø—Ä–æ—Å–∏–ª–∏)
    found_events_text = ""
    if is_event_query(current_message):
        raw_events = await db.get_potential_announcements(chat_id, days=30, limit=3)
        if raw_events:
            lines = [f"- {e.get('content')[:100]}..." for e in raw_events]
            found_events_text = "\n".join(lines)

    current_mood = determine_mood(current_message)
    persona = get_persona_prompt(current_mood, memory_text)
    
    task_instruction = "–û—Ç–≤–µ—Ç—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ. –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—É."
    
    if is_summary_query(current_message):
        task_instruction = f"–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É (summary) –ø–æ—Å–ª–µ–¥–Ω–∏—Ö {limit_history} —Å–æ–æ–±—â–µ–Ω–∏–π."
    elif is_event_query(current_message):
        if found_events_text:
            task_instruction = f"–ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –∞–Ω–æ–Ω—Å–æ–≤ –ø–æ–¥—Å–∫–∞–∂–∏, –∫—É–¥–∞ —Å—Ö–æ–¥–∏—Ç—å:\n{found_events_text}"
        else:
            task_instruction = "–ê–Ω–æ–Ω—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –û—Ç–≤–µ—Ç—å, —á—Ç–æ –ø–æ–∫–∞ –≥–ª—É—Ö–æ."

    system_prompt = f"{persona}\n\n–ó–ê–î–ê–ß–ê: {task_instruction}"
    messages = [{"role": "system", "content": system_prompt}]
    
    # –ò—Å—Ç–æ—Ä–∏—è (db.get_context —É–∂–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏, –Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π —Å—á–∏—Ç–∞–µ–º –∏–º–µ–Ω–Ω–æ —Ç–∞–∫)
    for row in history_rows:
        role = "assistant" if row["role"] == "model" else "user"
        content = clean_response(row.get("content"))
        name = row.get("user_name", "User")
        if content:
            msg_content = f"{name}: {content}" if role == "user" else content
            messages.append({"role": role, "content": msg_content})

    # –¢–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    user_msg_content = [{"type": "text", "text": current_message}]
    if image_data:
        try:
            buffered = io.BytesIO()
            image_data.save(buffered, format="JPEG", quality=80)
            b64 = base64.b64encode(buffered.getvalue()).decode("utf-8")
            user_msg_content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{b64}"},
            })
        except Exception:
            pass

    messages.append({"role": "user", "content": user_msg_content})

    # --- –≤—ã–±–æ—Ä –æ—á–µ—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞—á–∏ ---
    if is_summary_query(current_message) or is_event_query(current_message):
        # —Ñ–æ—Ä—Å–∏–º reasoning-–ª–∏–Ω–µ–π–∫—É
        queue = [
            AVAILABLE_MODELS["aurora-alpha"],
            AVAILABLE_MODELS["step-flash"],
            AVAILABLE_MODELS["deepseek-r1"],
            AVAILABLE_MODELS["lfm-instruct"],  # —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ–ª–ª–±–µ–∫ –Ω–∞ –∫—Ä–∞–π–Ω–∏–π —Å–ª—É—á–∞–π
        ]
    elif image_data:
        # —Ç–æ–ª—å–∫–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ, –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        queue = sorted(
            [m for m in AVAILABLE_MODELS.values() if m["multimodal"]],
            key=lambda x: x["priority"],
        )
    else:
        # –æ–±—ã—á–Ω—ã–π —á–∞—Ç: –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        queue = sorted(
            [m for m in AVAILABLE_MODELS.values() if not m["multimodal"]],
            key=lambda x: x["priority"],
        )

    # –ü–µ—Ä–µ–±–æ—Ä –º–æ–¥–µ–ª–µ–π –ø–æ –æ—á–µ—Ä–µ–¥–∏
    for model_cfg in queue:
        try:
            logging.info(f"‚ö° Trying {model_cfg['display_name']}...")
            
            response = await client.chat.completions.create(
                model=model_cfg["name"],
                messages=messages,
                temperature=0.7,
                max_tokens=1000,
                stream=False,
                extra_headers={
                    "HTTP-Referer": "https://telegram.org",
                    "X-Title": "Yachejka Bot",
                },
            )
            
            reply_text = clean_response(response.choices[0].message.content)
            
            if not reply_text or is_refusal(reply_text):
                logging.warning(f"‚ö†Ô∏è {model_cfg['display_name']} skipped (refusal/empty)")
                continue
            
            logging.info(f"‚úÖ Served by {model_cfg['display_name']}")
            return reply_text
            
        except Exception as e:
            logging.warning(f"‚ùå {model_cfg['display_name']} error: {e}")
            continue

    return "–ß—Ç–æ-—Ç–æ —è –∑–∞–≤–∏—Å–ª–∞... (–≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã)"
