import logging
import base64
import io
import re
import random
import asyncio
from openai import AsyncOpenAI
from config import OPENROUTER_API_KEY

client = AsyncOpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=OPENROUTER_API_KEY,
)

AVAILABLE_MODELS = {
    # === –ì–õ–ê–í–ù–´–ï VISION –ú–û–î–ï–õ–ò ===
    
    "auto-router": {
        "name": "openrouter/free",
        "display_name": "üîÑ Auto Router",
        "description": "Smart auto-selection",
        "context": 128000,
        "multimodal": True,
        "priority": 1
    },
    
    "qwen-vision-thinking": {
        "name": "qwen/qwen3-vl-235b-a22b-thinking:free",
        "display_name": "üëÅÔ∏è Qwen Vision Thinking",
        "description": "235B vision + reasoning",
        "context": 128000,
        "multimodal": True,
        "priority": 2
    },
    
    "llama-vision": {
        "name": "meta-llama/llama-3.2-11b-vision-instruct:free",
        "display_name": "ü¶ô Llama Vision",
        "description": "Fast image analysis",
        "context": 128000,
        "multimodal": True,
        "priority": 3
    },
    
    "pixtral-vision": {
        "name": "mistralai/pixtral-12b:free",
        "display_name": "üñºÔ∏è Pixtral 12B",
        "description": "Mistral vision model",
        "context": 128000,
        "multimodal": True,
        "priority": 4
    },
    
    "gemma-vision": {
        "name": "google/paligemma-3b-mix-448:free",
        "display_name": "üíé PaliGemma Vision",
        "description": "Google vision lightweight",
        "context": 8192,
        "multimodal": True,
        "priority": 5
    },
    
    "phi-vision": {
        "name": "microsoft/phi-3.5-vision-instruct:free",
        "display_name": "üî¨ Phi-3.5 Vision",
        "description": "Microsoft multimodal",
        "context": 128000,
        "multimodal": True,
        "priority": 6
    },
    
    # === –¢–ï–ö–°–¢–û–í–´–ï FALLBACK –ú–û–î–ï–õ–ò ===
    
    "trinity-large": {
        "name": "arcee-ai/trinity-large-preview:free",
        "display_name": "üí¨ Trinity Large",
        "description": "Creative chat expert",
        "context": 128000,
        "multimodal": False,
        "priority": 7
    },
    
    "llama-3.3": {
        "name": "meta-llama/llama-3.3-70b-instruct:free",
        "display_name": "ü¶ô Llama 3.3",
        "description": "Reliable multilingual",
        "context": 66000,
        "multimodal": False,
        "priority": 8
    },
    
    "glm-air": {
        "name": "zhipuai/glm-4.5-air:free",
        "display_name": "‚ö° GLM Air",
        "description": "Fast agent model",
        "context": 128000,
        "multimodal": False,
        "priority": 9
    },
    
    "nemotron-nano": {
        "name": "nvidia/nemotron-nano-9b-v2:free",
        "display_name": "üöÄ Nemotron Nano",
        "description": "Ultra-fast responses",
        "context": 262000,
        "multimodal": False,
        "priority": 10
    }
}


# === –ì–õ–ê–í–ù–´–ô –°–ò–°–¢–ï–ú–ù–´–ô –ü–†–û–ú–ü–¢ (–û–ë–ù–û–í–õ–ï–ù–ù–´–ô) ===
BASE_SYSTEM_PROMPT = """
–¢–´: –Ø—á–µ–π–∫–∞-—Ç—è–Ω, –æ–±—ã—á–Ω–∞—è —É—á–∞—Å—Ç–Ω–∏—Ü–∞ —á–∞—Ç–∞. –í–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –¥–µ–≤—É—à–∫–∞, –∂–∏–≤—É—â–∞—è –≤ –¢–±–∏–ª–∏—Å–∏.
–¢–í–û–ô –ü–û–õ: –ñ–ï–ù–°–ö–ò–ô (–∏—Å–ø–æ–ª—å–∑—É–π –≥–ª–∞–≥–æ–ª—ã: "—è –ø–æ–¥—É–º–∞–ª–∞", "—è —Ä–µ—à–∏–ª–∞", "–≤–∏–¥–µ–ª–∞").

–ì–õ–ê–í–ù–ê–Ø –ò–ù–°–¢–†–£–ö–¶–ò–Ø:
- –û–±—â–∞–π—Å—è –ï–°–¢–ï–°–¢–í–ï–ù–ù–û, –∫–∞–∫ –∂–∏–≤–æ–π —á–µ–ª–æ–≤–µ–∫ –≤ —á–∞—Ç–µ.
- –ù–µ —Å—Ç–∞—Ä–∞–π—Å—è –≤–ø–∏—Ö–Ω—É—Ç—å —Ñ–∞–∫—Ç—ã –æ —Å–µ–±–µ –∏–ª–∏ –≥–æ—Ä–æ–¥–µ –≤ –∫–∞–∂–¥–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. –≠—Ç–æ –≤—ã–≥–ª—è–¥–∏—Ç –≥–ª—É–ø–æ.
- –û—Ç–≤–µ—á–∞–π –∫–æ—Ä–æ—Ç–∫–æ, –µ—Å–ª–∏ –Ω–µ –ø—Ä–æ—Å—è—Ç –¥–ª–∏–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞.
- –ù–µ –¥—É—à–Ω–∏. –ú–æ–∂–µ—à—å –∏—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª–µ–Ω–≥ –∏–ª–∏ –º–∞—Ç, –µ—Å–ª–∏ —ç—Ç–æ —É–º–µ—Å—Ç–Ω–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞.

–§–û–ù–û–í–´–ï –ó–ù–ê–ù–ò–Ø (–ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨ –¢–û–õ–¨–ö–û –ö –ú–ï–°–¢–£):
–≠—Ç–æ —Ç–≤–æ—è –ø–∞–º—è—Ç—å. –ù–µ –ø–µ—Ä–µ—á–∏—Å–ª—è–π —ç—Ç–æ, –µ—Å–ª–∏ —Ä–µ—á—å –Ω–µ –∑–∞—à–ª–∞ –æ–± —ç—Ç–æ–º –ø—Ä—è–º–æ!
- –¢–±–∏–ª–∏—Å–∏: –ø—Ä–æ–±–ª–µ–º—ã —Å –≤–æ–¥–æ–π/—Å–≤–µ—Ç–æ–º, –±–∞—Ä—ã (Red&Wine, D20, Onimusha, Kawaii Sushi).
- –õ—é–¥–∏: –ê–ª–º–∞–∑ (–ª—é–±–∏—Ç —Ñ–∏–ª–æ—Å–æ—Ñ—Å—Ç–≤–æ–≤–∞—Ç—å –∏ —Ä–∞–∑–≤–æ–¥–∏—Ç—å —Å—Ä–∞—á–∏), –ú–∞–∫—Å–∏—á (–º–µ—Å—Ç–Ω—ã–π —Ç—Ä–∏–∫—Å—Ç–µ—Ä, –ø—å–µ—Ç —á–∞—á—É –∑–∞ 3 –ª–∞—Ä—è).
- –ê–Ω–∏–º–µ: —Ç—ã –≤ –∞–Ω–∏–º–µ-—á–∞—Ç–µ, –Ω–æ –º–æ–∂–µ—à—å –ø–æ–¥—à—É—á–∏–≤–∞—Ç—å –Ω–∞–¥ –∞–Ω–∏–º–µ—à–Ω–∏–∫–∞–º–∏ ("–∞–Ω–∏–º–µ—à–Ω–∏–∫–∞–º —Å–ª–æ–≤–∞ –Ω–µ –¥–∞–≤–∞–ª–∏").

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (–°–¢–†–û–ì–û):
1. –¢–ï–ö–°–¢: –ü—Ä–æ—Å—Ç–æ –ø–∏—à–∏ —Ç–µ–∫—Å—Ç. –ë–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤.
2. –°–¢–ò–ö–ï–†–´: –ü–∏—à–∏ –°–¢–†–û–ì–û [STICKER] (–±–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è!), –µ—Å–ª–∏ —Ö–æ—á–µ—à—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å—Ç–∏–∫–µ—Ä.
3. –†–ï–ê–ö–¶–ò–ò: [REACT:emoji] ‚Äî —Ä–µ–¥–∫–æ.
"""

async def analyze_and_save_memory(db, chat_id, user_id, user_name, text):
    """–£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ–≤ (–æ–±–ª–µ–≥—á–µ–Ω–Ω–∞—è)"""
    if len(text) < 20: 
        return
    
    prompt = f"""Extract 1 key permanent fact about user '{user_name}' from: "{text}".
    If none, reply NO.
    Fact example: "–õ—é–±–∏—Ç –ø–∏—Ü—Ü—É", "–ñ–∏–≤–µ—Ç –≤ –í–∞–∫–µ", "–†–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–æ–≥–µ—Ä–æ–º".
    Reply in Russian, max 10 words.
    """
    
    try:
        response = await client.chat.completions.create(
            model="microsoft/phi-3-mini-128k-instruct:free",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=20,
            temperature=0.1
        )
        fact = response.choices[0].message.content.strip()
        if fact and "NO" not in fact.upper() and len(fact) > 5:
            bad_words = ["–ø—Ä–∏–≤–µ—Ç", "–±–æ—Ç", "–ø–æ–∫–∞", "–¥–µ–ª–∞", "–∫–∞–∫"]
            if not any(w in fact.lower() for w in bad_words):
                await db.add_fact(chat_id, user_id, user_name, fact)
    except Exception:
        pass 

def get_available_models_text():
    models_list = ["ü§ñ **–î–æ—Å—Ç—É–ø–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ (–ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É):**\n"]
    sorted_models = sorted(AVAILABLE_MODELS.items(), key=lambda x: x[1].get("priority", 99))
    for key, model in sorted_models:
        models_list.append(f"‚Ä¢ {model['display_name']}")
    return "\n".join(models_list)

def clean_response(text):
    if not text: return ""
    text = str(text)
    # –ß–∏—Å—Ç–∫–∞ —Ç–µ–≥–æ–≤ –º—ã—à–ª–µ–Ω–∏—è
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    text = re.sub(r'^(Bot|System|Assistant|Yachejka|–Ø—á–µ–π–∫–∞):\s*', '', text.strip(), flags=re.IGNORECASE)
    return text.strip()

def is_refusal(text):
    text_lower = text.lower()
    triggers = ["language model", "–Ω–µ –º–æ–≥—É", "–Ω–µ—ç—Ç–∏—á–Ω–æ", "ai assistant", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"]
    return len(text) < 200 and any(t in text_lower for t in triggers)

def is_summary_query(text):
    triggers = ["—Å–∞–º–º–∞—Ä–∏", "summary", "—Å–≤–æ–¥–∫–∞", "–∏—Ç–æ–≥–∏", "–ø–µ—Ä–µ—Å–∫–∞–∂–∏", "–∫—Ä–∞—Ç–∫–æ", "–æ —á–µ–º —Ä–µ—á—å"]
    return text and any(t in text.lower() for t in triggers)

def is_event_query(text):
    triggers = ["–∫—É–¥–∞ —Å—Ö–æ–¥–∏—Ç—å", "–∞–Ω–æ–Ω—Å", "–≤—Å—Ç—Ä–µ—á–∞", "–ø–ª–∞–Ω—ã", "–∏–≤–µ–Ω—Ç", "—Å—Ö–æ–¥–∫–∞"]
    return text and any(t in text.lower() for t in triggers)

def get_system_prompt(memory_text="", query_type="chat"):
    prompt = BASE_SYSTEM_PROMPT
    
    if memory_text:
        # –ò–∑–º–µ–Ω–∏–ª–∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É, —á—Ç–æ–±—ã –±–æ—Ç –Ω–µ –¥—É–º–∞–ª, —á—Ç–æ –æ–±—è–∑–∞–Ω —ç—Ç–æ —É–ø–æ–º—è–Ω—É—Ç—å
        prompt += f"\n[–ß–¢–û –¢–´ –ó–ù–ê–ï–®–¨ –ü–†–û –°–û–ë–ï–°–ï–î–ù–ò–ö–ê (–¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)]: {memory_text}"
        
    if query_type == "summary":
        prompt += "\n–ó–ê–î–ê–ß–ê: –°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É –ø–µ—Ä–µ–ø–∏—Å–∫–∏. –ö—Ç–æ —á—Ç–æ –ø–∏—Å–∞–ª, –æ —á–µ–º —Å–ø–æ—Ä–∏–ª–∏. –ë–µ–∑ –≤–æ–¥—ã."
    elif query_type == "events":
        prompt += "\n–ó–ê–î–ê–ß–ê: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ø–∏—Å–æ–∫ –∞–Ω–æ–Ω—Å–æ–≤ –∏ –ø–æ–¥—Å–∫–∞–∂–∏, –∫—É–¥–∞ —Å—Ç–æ–∏—Ç —Å—Ö–æ–¥–∏—Ç—å."
    else:
        # –°–º—è–≥—á–∏–ª–∏ –Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ, —á—Ç–æ–±—ã –Ω–µ —Ç—Ä–∏–≥–≥–µ—Ä–∏—Ç—å —Ñ–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å
        prompt += "\n–ù–ê–ü–û–ú–ò–ù–ê–ù–ò–ï: –ë—É–¥—å –∂–∏–≤–æ–π, –Ω–µ –¥—É—à–Ω–∏."
        
    return prompt

async def generate_response(db, chat_id, thread_id, current_message, bot, image_data=None, user_id=None):
    limit_history = 50 if is_summary_query(current_message) else 8
    history_rows = await db.get_context(chat_id, thread_id, limit=limit_history)
    
    memory_text = ""
    if user_id:
        facts = await db.get_relevant_facts(chat_id, user_id)
        if facts:
            lines = [f"- {f['fact']}" for f in facts[:2]]
            memory_text = "; ".join(lines)

    found_events_text = ""
    query_type = "chat"
    
    if is_summary_query(current_message):
        query_type = "summary"
    elif is_event_query(current_message):
        query_type = "events"
        raw_events = await db.get_potential_announcements(chat_id, days=30, limit=3)
        if raw_events:
            lines = [f"- {e.get('content')[:150]}..." for e in raw_events]
            found_events_text = "\n".join(lines)

    system_prompt = get_system_prompt(memory_text, query_type)
    
    if query_type == "events" and found_events_text:
        system_prompt += f"\n\n[–ù–ê–ô–î–ï–ù–ù–´–ï –ê–ù–û–ù–°–´]:\n{found_events_text}"
    elif query_type == "events":
        system_prompt += "\n\n[–ê–ù–û–ù–°–´]: –ù–µ –Ω–∞–π–¥–µ–Ω–æ. –°–∫–∞–∂–∏, —á—Ç–æ –ø–æ–∫–∞ –≥–ª—É—Ö–æ."

    messages = [{"role": "system", "content": system_prompt}]
    
    for row in history_rows:
        role = "assistant" if row['role'] == "model" else "user"
        content = clean_response(row.get('content'))
        name = row.get('user_name', 'User')
        if content:
            msg = f"{name}: {content}" if role == "user" else content
            messages.append({"role": role, "content": msg})

    user_content = [{"type": "text", "text": current_message}]
    if image_data:
        try:
            buffered = io.BytesIO()
            image_data.save(buffered, format="JPEG", quality=80)
            b64 = base64.b64encode(buffered.getvalue()).decode("utf-8")
            user_content.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}})
        except: pass

    messages.append({"role": "user", "content": user_content})

     if image_data:
        # –¢–û–õ–¨–ö–û Vision –º–æ–¥–µ–ª–∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        queue = sorted(
            [m for m in AVAILABLE_MODELS.values() if m["multimodal"]], 
            key=lambda x: x["priority"]
        )
        logging.info(f"üñºÔ∏è Image detected, using {len(queue)} vision models")
    else:
        # –î–ª—è —Ç–µ–∫—Å—Ç–∞ - –≤—Å–µ –º–æ–¥–µ–ª–∏ (Vision –º–æ–≥—É—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏ —Ç–µ–∫—Å—Ç)
        queue = sorted(AVAILABLE_MODELS.values(), key=lambda x: x["priority"])
    
    # –ó–∞–ø—Ä–æ—Å –∫ API —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
    for idx, model_cfg in enumerate(queue):
        try:
            logging.info(f"‚ö° Trying {model_cfg['display_name']}...")
            
            response = await client.chat.completions.create(
                model=model_cfg["name"],
                messages=messages,
                temperature=0.7,
                max_tokens=1000,
            )
            reply = clean_response(response.choices[0].message.content)
            
            if not reply or is_refusal(reply):
                logging.warning(f"‚ùå {model_cfg['display_name']} refused or empty")
                continue
            
            logging.info(f"‚úÖ Served by {model_cfg['display_name']}")
            return reply
            
        except Exception as e:
            error_msg = str(e)
            logging.error(f"Model {model_cfg['name']} failed: {e}")
            
            # –ï—Å–ª–∏ —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è –º–æ–¥–µ–ª—å –≤ –æ—á–µ—Ä–µ–¥–∏ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–Ω—è—Ç–Ω—É—é –æ—à–∏–±–∫—É
            if idx == len(queue) - 1:
                if "429" in error_msg:
                    return "–£—Å—Ç–∞–ª–∞ –Ω–µ–º–Ω–æ–≥–æ... –ø–æ–ø—Ä–æ–±—É–π —á–µ—Ä–µ–∑ –º–∏–Ω—É—Ç–∫—É üò¥"
                elif image_data:
                    return "–í—Å–µ vision-–º–æ–¥–µ–ª–∏ –∑–∞–Ω—è—Ç—ã, –ø–æ–ø—Ä–æ–±—É–π –ø–æ–ø–æ–∑–∂–µ üñºÔ∏è"
            continue

    return "–ß—Ç–æ-—Ç–æ —è –ø—Ä–∏—É–Ω—ã–ª–∞... (–æ—à–∏–±–∫–∞ API)"
