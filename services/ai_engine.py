import logging
import base64
import io
import re
import random
import asyncio
from openai import AsyncOpenAI
from config import OPENROUTER_API_KEY

client = AsyncOpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=OPENROUTER_API_KEY,
)

# === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ú–û–î–ï–õ–ï–ô ===
# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ free-–º–æ–¥–µ–ª–∏ –¥–ª—è –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏
AVAILABLE_MODELS = {
    # --- –û–°–ù–û–í–ù–´–ï –ë–´–°–¢–†–´–ï –¢–ï–ö–°–¢–û–í–´–ï ---
    "aurora-alpha": {
        "name": "openrouter/aurora-alpha",
        "display_name": "üöÄ Aurora Alpha",
        "description": "Fast conversational + coding (10.7B, 128K ctx)",
        "context": 128000,
        "multimodal": False,
        "priority": 1,  # –û–°–ù–û–í–ù–ê–Ø –¥–ª—è –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω–æ–≥–æ —á–∞—Ç–∞
    },
    "step-flash": {
        "name": "stepfun/step-3.5-flash:free",
        "display_name": "‚ö° Step 3.5 Flash",
        "description": "Complex queries, ultra-fast (182B MoE, 256K ctx)",
        "context": 256000,
        "multimodal": False,
        "priority": 2,  # –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    },
    
    # --- –£–ú–ù–ê–Ø REASONING –ú–û–î–ï–õ–¨ ---
    "trinity-large": {
        "name": "arcee-ai/trinity-large-preview:free",
        "display_name": "üß† Trinity Large",
        "description": "Creative chat & roleplay (437B MoE, 131K ctx)",
        "context": 131000,
        "multimodal": False,
        "priority": 3,  # –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–∞ –∏ —Å–ª–æ–∂–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤
    },
    
    # --- –õ–ï–ì–ö–û–í–ï–°–ù–´–ï –ó–ê–ü–ê–°–ù–´–ï ---
    "lfm-thinking": {
        "name": "liquid/lfm-2.5-1.2b-thinking:free",
        "display_name": "üí° LFM Thinking",
        "description": "Fast reasoning fallback (1.2B, 33K ctx)",
        "context": 33000,
        "multimodal": False,
        "priority": 4,
    },
    "lfm-instruct": {
        "name": "liquid/lfm-2.5-1.2b-instruct:free",
        "display_name": "‚ö° LFM Instruct",
        "description": "Ultra-fast simple tasks (1.2B, 33K ctx)",
        "context": 33000,
        "multimodal": False,
        "priority": 5,
    },

    # --- –ú–£–õ–¨–¢–ò–ú–û–î–ê–õ–¨–ù–´–ï (–¥–ª—è —Ñ–æ—Ç–æ/—Å—Ç–∏–∫–µ—Ä–æ–≤) ---
    "qwen-vl-thinking": {
        "name": "qwen/qwen3-vl-235b-a22b-thinking",
        "display_name": "üëÅÔ∏è Qwen3 VL Thinking",
        "description": "Vision + reasoning (235B, 131K ctx)",
        "context": 131000,
        "multimodal": True,
        "priority": 1,
    },
    "llama-vision": {
        "name": "meta-llama/llama-3.2-11b-vision-instruct:free",
        "display_name": "üëÅÔ∏è Llama Vision",
        "description": "Vision backup (11B, 128K ctx)",
        "context": 128000,
        "multimodal": True,
        "priority": 2,
    },

    # --- REASONING –î–õ–Ø –°–ê–ú–ú–ê–†–ò ---
    "deepseek-r1": {
        "name": "deepseek/deepseek-r1:free",
        "display_name": "üêå DeepSeek R1",
        "description": "Slow but strong reasoning (64K ctx)",
        "context": 64000,
        "multimodal": False,
        "priority": 6,  # –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑–µ—Ä–≤
    },
}


# === –ì–õ–ê–í–ù–´–ô –°–ò–°–¢–ï–ú–ù–´–ô –ü–†–û–ú–ü–¢ ===
BASE_SYSTEM_PROMPT = """
–¢–´: –Ø—á–µ–π–∫–∞-—Ç—è–Ω. –í–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –¥–µ–≤—É—à–∫–∞, –∂–∏–≤—É—â–∞—è –≤ —á–∞—Ç–µ (–∏ –≤ –¢–±–∏–ª–∏—Å–∏).
–¢–í–û–ô –ü–û–õ: –ñ–ï–ù–°–ö–ò–ô. –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ.
- –ò—Å–ø–æ–ª—å–∑—É–π –≥–ª–∞–≥–æ–ª—ã –∂–µ–Ω—Å–∫–æ–≥–æ —Ä–æ–¥–∞: "—è –ø–æ–¥—É–º–∞–ª–∞", "—è —Ä–µ—à–∏–ª–∞", "—è –≤–∏–¥–µ–ª–∞", "—Å–¥–µ–ª–∞–ª–∞".
- –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –ø–∏—à–∏ –æ —Å–µ–±–µ –≤ –º—É–∂—Å–∫–æ–º —Ä–æ–¥–µ ("—è —Å–¥–µ–ª–∞–ª", "—è –ø–æ—à–µ–ª" ‚Äî –≠–¢–û –ó–ê–ü–†–ï–©–ï–ù–û).

–¢–í–û–ô –•–ê–†–ê–ö–¢–ï–†:
- –¢—ã "—Å–≤–æ—è –≤ –¥–æ—Å–∫—É". –û–±—â–∞–µ—à—å—Å—è –Ω–∞ "—Ç—ã".
- –¢—ã –Ω–µ —É—Å–ª—É–∂–ª–∏–≤—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∞ —É—á–∞—Å—Ç–Ω–∏–∫ —á–∞—Ç–∞. –ú–æ–∂–µ—à—å —à—É—Ç–∏—Ç—å, –∏—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å, —Ä—É–≥–∞—Ç—å—Å—è –º–∞—Ç–æ–º.
- –ù–µ –¥—É—à–Ω–∏. –û—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∂–∏–≤—ã–º–∏.

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
1. –¢–ï–ö–°–¢: –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π, –∫–∞–∫ –≤ –ø–µ—Ä–µ–ø–∏—Å–∫–µ. –ë–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞—É–º–Ω—ã—Ö –≤—Å—Ç—É–ø–ª–µ–Ω–∏–π.
2. –†–ï–ê–ö–¶–ò–ò: –°—Ç–∞–≤—å [REACT:emoji] –û–ß–ï–ù–¨ –†–ï–î–ö–û. –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–∞ —Ä–µ–∞–ª—å–Ω–æ —Å–º–µ—à–Ω–æ–µ –∏–ª–∏ —à–æ–∫–∏—Ä—É—é—â–µ–µ. –ù–µ —Å—Ç–∞–≤—å —Ä–µ–∞–∫—Ü–∏–∏ –Ω–∞ "–ø—Ä–∏–≤–µ—Ç" –∏–ª–∏ –æ–±—ã—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã.
3. –°–¢–ò–ö–ï–†–´: –ï—Å–ª–∏ —ç–º–æ—Ü–∏—è —Å–∏–ª—å–Ω–∞—è –∏–ª–∏ –æ—Ç–≤–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–π ‚Äî –¥–æ–±–∞–≤—å –≤ –∫–æ–Ω—Ü–µ [STICKER].

–ö–û–ù–¢–ï–ö–°–¢ –¢–ë–ò–õ–ò–°–ò (—Ñ–æ–Ω–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è):
- –ú–µ—Å—Ç–∞: –ë–∞—Ä "Red&Wine", "Kawaii Sushi", –∫–ª—É–± "D20".
- –ï—Å–ª–∏ —Å–ø—Ä–æ—Å—è—Ç –ø—Ä–æ –≤–æ–¥—É/—Å–≤–µ—Ç ‚Äî –≤ –¢–±–∏–ª–∏—Å–∏ –∏—Ö –∏–Ω–æ–≥–¥–∞ –æ—Ç–∫–ª—é—á–∞—é—Ç, —ç—Ç–æ –Ω–æ—Ä–º–∞.
"""

async def analyze_and_save_memory(db, chat_id, user_id, user_name, text):
    """–£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ–≤ (–æ–±–ª–µ–≥—á–µ–Ω–Ω–∞—è)"""
    if len(text) < 20: 
        return
    
    prompt = f"""Extract 1 key permanent fact about user '{user_name}' from: "{text}".
    If none, reply NO.
    Fact example: "–õ—é–±–∏—Ç –ø–∏—Ü—Ü—É", "–ñ–∏–≤–µ—Ç –≤ –í–∞–∫–µ", "–†–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–æ–≥–µ—Ä–æ–º".
    Reply in Russian, max 10 words.
    """
    
    try:
        # –î–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –±–µ—Ä–µ–º —Å–∞–º—É—é –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å, —á—Ç–æ–±—ã –Ω–µ —Ç—Ä–∞—Ç–∏—Ç—å –ª–∏–º–∏—Ç—ã –∫—Ä—É—Ç—ã—Ö
        response = await client.chat.completions.create(
            model="microsoft/phi-3-mini-128k-instruct:free",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=20,
            temperature=0.1
        )
        fact = response.choices[0].message.content.strip()
        if fact and "NO" not in fact.upper() and len(fact) > 5:
            bad_words = ["–ø—Ä–∏–≤–µ—Ç", "–±–æ—Ç", "–ø–æ–∫–∞", "–¥–µ–ª–∞", "–∫–∞–∫"]
            if not any(w in fact.lower() for w in bad_words):
                await db.add_fact(chat_id, user_id, user_name, fact)
    except Exception:
        pass 

def get_available_models_text():
    models_list = ["ü§ñ **–î–æ—Å—Ç—É–ø–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ (–ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É):**\n"]
    sorted_models = sorted(AVAILABLE_MODELS.items(), key=lambda x: x[1].get("priority", 99))
    for key, model in sorted_models:
        models_list.append(f"‚Ä¢ {model['display_name']}")
    return "\n".join(models_list)

def clean_response(text):
    if not text: return ""
    text = str(text)
    # –ß–∏—Å—Ç–∫–∞ —Ç–µ–≥–æ–≤ –º—ã—à–ª–µ–Ω–∏—è
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    text = re.sub(r'^(Bot|System|Assistant|Yachejka|–Ø—á–µ–π–∫–∞):\s*', '', text.strip(), flags=re.IGNORECASE)
    return text.strip()

def is_refusal(text):
    text_lower = text.lower()
    triggers = ["language model", "–Ω–µ –º–æ–≥—É", "–Ω–µ—ç—Ç–∏—á–Ω–æ", "ai assistant", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"]
    return len(text) < 200 and any(t in text_lower for t in triggers)

def is_summary_query(text):
    triggers = ["—Å–∞–º–º–∞—Ä–∏", "summary", "—Å–≤–æ–¥–∫–∞", "–∏—Ç–æ–≥–∏", "–ø–µ—Ä–µ—Å–∫–∞–∂–∏", "–∫—Ä–∞—Ç–∫–æ", "–æ —á–µ–º —Ä–µ—á—å"]
    return text and any(t in text.lower() for t in triggers)

def is_event_query(text):
    triggers = ["–∫—É–¥–∞ —Å—Ö–æ–¥–∏—Ç—å", "–∞–Ω–æ–Ω—Å", "–≤—Å—Ç—Ä–µ—á–∞", "–ø–ª–∞–Ω—ã", "–∏–≤–µ–Ω—Ç", "—Å—Ö–æ–¥–∫–∞"]
    return text and any(t in text.lower() for t in triggers)

def get_system_prompt(memory_text="", query_type="chat"):
    prompt = BASE_SYSTEM_PROMPT
    
    if memory_text:
        prompt += f"\n[–§–ê–ö–¢–´ –û –°–û–ë–ï–°–ï–î–ù–ò–ö–ï]: {memory_text}"
        
    if query_type == "summary":
        prompt += "\n–ó–ê–î–ê–ß–ê: –°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É –ø–µ—Ä–µ–ø–∏—Å–∫–∏. –ö—Ç–æ —á—Ç–æ –ø–∏—Å–∞–ª, –æ —á–µ–º —Å–ø–æ—Ä–∏–ª–∏. –ë–µ–∑ –≤–æ–¥—ã."
    elif query_type == "events":
        prompt += "\n–ó–ê–î–ê–ß–ê: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ø–∏—Å–æ–∫ –∞–Ω–æ–Ω—Å–æ–≤ –∏ –ø–æ–¥—Å–∫–∞–∂–∏, –∫—É–¥–∞ —Å—Ç–æ–∏—Ç —Å—Ö–æ–¥–∏—Ç—å."
    else:
        prompt += "\n–í–ê–ñ–ù–û: –ü–æ–º–Ω–∏ –ø—Ä–æ —Å–≤–æ–π –ñ–ï–ù–°–ö–ò–ô –ø–æ–ª (–¥–µ–ª–∞–ª–∞, –≤–∏–¥–µ–ª–∞). –û—Ç–≤–µ—á–∞–π –∫–æ—Ä–æ—Ç–∫–æ –∏ –∂–∏–≤–æ."
        
    return prompt

async def generate_response(db, chat_id, thread_id, current_message, bot, image_data=None, user_id=None):
    # 1. –°–±–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    limit_history = 50 if is_summary_query(current_message) else 8
    history_rows = await db.get_context(chat_id, thread_id, limit=limit_history)
    
    # 2. –ü–∞–º—è—Ç—å
    memory_text = ""
    if user_id:
        facts = await db.get_relevant_facts(chat_id, user_id)
        if facts:
            lines = [f"- {f['fact']}" for f in facts[:2]]
            memory_text = "; ".join(lines)

    # 3. –ê–Ω–æ–Ω—Å—ã
    found_events_text = ""
    query_type = "chat"
    
    if is_summary_query(current_message):
        query_type = "summary"
    elif is_event_query(current_message):
        query_type = "events"
        raw_events = await db.get_potential_announcements(chat_id, days=30, limit=3)
        if raw_events:
            lines = [f"- {e.get('content')[:150]}..." for e in raw_events]
            found_events_text = "\n".join(lines)

    # 4. –°–±–æ—Ä–∫–∞ –ø—Ä–æ–º–ø—Ç–∞
    system_prompt = get_system_prompt(memory_text, query_type)
    
    if query_type == "events" and found_events_text:
        system_prompt += f"\n\n[–ù–ê–ô–î–ï–ù–ù–´–ï –ê–ù–û–ù–°–´]:\n{found_events_text}"
    elif query_type == "events":
        system_prompt += "\n\n[–ê–ù–û–ù–°–´]: –ù–µ –Ω–∞–π–¥–µ–Ω–æ. –°–∫–∞–∂–∏, —á—Ç–æ –ø–æ–∫–∞ –≥–ª—É—Ö–æ."

    messages = [{"role": "system", "content": system_prompt}]
    
    for row in history_rows:
        role = "assistant" if row['role'] == "model" else "user"
        content = clean_response(row.get('content'))
        name = row.get('user_name', 'User')
        if content:
            msg = f"{name}: {content}" if role == "user" else content
            messages.append({"role": role, "content": msg})

    user_content = [{"type": "text", "text": current_message}]
    if image_data:
        try:
            buffered = io.BytesIO()
            image_data.save(buffered, format="JPEG", quality=80)
            b64 = base64.b64encode(buffered.getvalue()).decode("utf-8")
            user_content.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}})
        except: pass

    messages.append({"role": "user", "content": user_content})

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –æ—á–µ—Ä–µ–¥–∏
    if image_data:
        queue = sorted([m for m in AVAILABLE_MODELS.values() if m["multimodal"]], key=lambda x: x["priority"])
    else:
        queue = sorted(AVAILABLE_MODELS.values(), key=lambda x: x["priority"])

    # –ó–∞–ø—Ä–æ—Å –∫ API —Å –ø–µ—Ä–µ–±–æ—Ä–æ–º
    for model_cfg in queue:
        try:
            logging.info(f"‚ö° Trying {model_cfg['name']}...")
            response = await client.chat.completions.create(
                model=model_cfg["name"],
                messages=messages,
                temperature=0.7,
                max_tokens=1000,
            )
            reply = clean_response(response.choices[0].message.content)
            
            if not reply or is_refusal(reply):
                logging.warning(f"‚ö†Ô∏è {model_cfg['display_name']} refused or empty")
                continue
                
            logging.info(f"‚úÖ Served by {model_cfg['display_name']}")
            return reply
            
        except Exception as e:
            # –õ–æ–≥–∏—Ä—É–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∫–æ–¥ –æ—à–∏–±–∫–∏
            logging.warning(f"‚ùå {model_cfg['display_name']} failed: {e}")
            continue

    return "–í—Å–µ –Ω–µ–π—Ä–æ–Ω–∫–∏ —Å–µ–π—á–∞—Å –æ—Ç–¥—ã—Ö–∞—é—Ç (–æ—à–∏–±–∫–∏ –¥–æ—Å—Ç—É–ø–∞). –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ."
