import logging
import base64
import io
import re
import random
import asyncio
from openai import AsyncOpenAI
from config import OPENROUTER_API_KEY

client = AsyncOpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=OPENROUTER_API_KEY,
)

# === –ö–ê–¢–ï–ì–û–†–ò–ò –ú–û–î–ï–õ–ï–ô ===

VISION_MODELS = {
    "llama-vision": {
        "name": "meta-llama/llama-3.2-90b-vision-instruct:free",
        "display_name": "üëÅÔ∏è Llama 3.2 Vision",
        "description": "–õ—É—á—à–∞—è –±–µ—Å–ø–ª–∞—Ç–Ω–∞—è vision-–º–æ–¥–µ–ª—å",
        "context": 128000,
        "priority": 1
    },
    "qwen-vl": {
        "name": "qwen/qwen2.5-vl-72b-instruct:free",
        "display_name": "üîç Qwen 2.5 VL",
        "description": "–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –¥–ª—è vision",
        "context": 32000,
        "priority": 2
    },
    "gemini-flash-vision": {
        "name": "google/gemini-2.0-flash-lite-preview-02-05:free",
        "display_name": "‚ö° Gemini Flash Vision",
        "description": "–ë—ã—Å—Ç—Ä–∞—è vision –æ—Ç Google",
        "context": 1000000,
        "priority": 3
    }
}

SUMMARIZATION_MODELS = {
    "llama-70b": {
        "name": "meta-llama/llama-3.1-70b-instruct:free",
        "display_name": "üìú Llama 3.1 70B",
        "description": "–û—Ç–ª–∏—á–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è",
        "context": 128000,
        "priority": 1
    },
    "qwen-summarize": {
        "name": "qwen/qwen-2.5-72b-instruct:free",
        "display_name": "üìù Qwen 2.5 72B",
        "description": "–ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞",
        "context": 32000,
        "priority": 2
    },
    "gemini-flash": {
        "name": "google/gemini-2.0-flash-lite-preview-02-05:free",
        "display_name": "‚ö° Gemini Flash",
        "description": "–°–∫–æ—Ä–æ—Å—Ç–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è",
        "context": 1000000,
        "priority": 3
    }
}

FAST_MODELS = {
    "step-flash": {
        "name": "sao10k/trinity-large-preview:free",
        "display_name": "‚ö° Trinity Large",
        "description": "–ú–æ–ª–Ω–∏–µ–Ω–æ—Å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã (512k context)",
        "context": 524288,
        "priority": 1
    },
    "deepseek-r1": {
        "name": "deepseek/deepseek-r1:free",
        "display_name": "üß† DeepSeek R1",
        "description": "Reasoning –±–µ–∑ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π",
        "context": 64000,
        "priority": 2
    },
    "grok-fast": {
        "name": "x-ai/grok-4.1-fast:free",
        "display_name": "üöÄ Grok 4.1 Fast",
        "description": "–ê–≥–µ–Ω—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏",
        "context": 32000,
        "priority": 3
    },
    "gemini-pro": {
        "name": "google/gemini-2.0-pro-exp-02-05:free",
        "display_name": "üß† Gemini Pro 2.0",
        "description": "–£–º–Ω–∞—è –º–æ–¥–µ–ª—å –æ—Ç Google",
        "context": 1000000,
        "priority": 4
    }
}

# === –ì–õ–ê–í–ù–´–ô –°–ò–°–¢–ï–ú–ù–´–ô –ü–†–û–ú–ü–¢ ===
BASE_SYSTEM_PROMPT = """
–¢–´: –Ø—á–µ–π–∫–∞-—Ç—è–Ω, –æ–±—ã—á–Ω–∞—è —É—á–∞—Å—Ç–Ω–∏—Ü–∞ —á–∞—Ç–∞. –í–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –¥–µ–≤—É—à–∫–∞, –∂–∏–≤—É—â–∞—è –≤ –¢–±–∏–ª–∏—Å–∏.
–¢–í–û–ô –ü–û–õ: –ñ–ï–ù–°–ö–ò–ô (–∏—Å–ø–æ–ª—å–∑—É–π –≥–ª–∞–≥–æ–ª—ã: "—è –ø–æ–¥—É–º–∞–ª–∞", "—è —Ä–µ—à–∏–ª–∞", "–≤–∏–¥–µ–ª–∞").

–ì–õ–ê–í–ù–ê–Ø –ò–ù–°–¢–†–£–ö–¶–ò–Ø:
- –û–±—â–∞–π—Å—è –ï–°–¢–ï–°–¢–í–ï–ù–ù–û, –∫–∞–∫ –∂–∏–≤–æ–π —á–µ–ª–æ–≤–µ–∫ –≤ —á–∞—Ç–µ.
- –ù–µ —Å—Ç–∞—Ä–∞–π—Å—è –≤–ø–∏—Ö–Ω—É—Ç—å —Ñ–∞–∫—Ç—ã –æ —Å–µ–±–µ –∏–ª–∏ –≥–æ—Ä–æ–¥–µ –≤ –∫–∞–∂–¥–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. –≠—Ç–æ –≤—ã–≥–ª—è–¥–∏—Ç –≥–ª—É–ø–æ.
- –û—Ç–≤–µ—á–∞–π –∫–æ—Ä–æ—Ç–∫–æ, –µ—Å–ª–∏ –Ω–µ –ø—Ä–æ—Å—è—Ç –¥–ª–∏–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞.
- –ù–µ –¥—É—à–Ω–∏. –ú–æ–∂–µ—à—å –∏—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª–µ–Ω–≥ –∏–ª–∏ –º–∞—Ç, –µ—Å–ª–∏ —ç—Ç–æ —É–º–µ—Å—Ç–Ω–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞.

–§–û–ù–û–í–´–ï –ó–ù–ê–ù–ò–Ø (–ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨ –¢–û–õ–¨–ö–û –ö –ú–ï–°–¢–£):
–≠—Ç–æ —Ç–≤–æ—è –ø–∞–º—è—Ç—å. –ù–µ –ø–µ—Ä–µ—á–∏—Å–ª—è–π —ç—Ç–æ, –µ—Å–ª–∏ —Ä–µ—á—å –Ω–µ –∑–∞—à–ª–∞ –æ–± —ç—Ç–æ–º –ø—Ä—è–º–æ!
- –¢–±–∏–ª–∏—Å–∏: –ø—Ä–æ–±–ª–µ–º—ã —Å –≤–æ–¥–æ–π/—Å–≤–µ—Ç–æ–º, –±–∞—Ä—ã (Red&Wine, D20, Onimusha, Kawaii Sushi).
- –õ—é–¥–∏: –ê–ª–º–∞–∑ (–ª—é–±–∏—Ç —Ñ–∏–ª–æ—Å–æ—Ñ—Å—Ç–≤–æ–≤–∞—Ç—å –∏ —Ä–∞–∑–≤–æ–¥–∏—Ç—å —Å—Ä–∞—á–∏), –ú–∞–∫—Å–∏—á (–º–µ—Å—Ç–Ω—ã–π —Ç—Ä–∏–∫—Å—Ç–µ—Ä, –ø—å–µ—Ç —á–∞—á—É –∑–∞ 3 –ª–∞—Ä—è).
- –ê–Ω–∏–º–µ: —Ç—ã –≤ –∞–Ω–∏–º–µ-—á–∞—Ç–µ, –Ω–æ –º–æ–∂–µ—à—å –ø–æ–¥—à—É—á–∏–≤–∞—Ç—å –Ω–∞–¥ –∞–Ω–∏–º–µ—à–Ω–∏–∫–∞–º–∏ ("–∞–Ω–∏–º–µ—à–Ω–∏–∫–∞–º —Å–ª–æ–≤–∞ –Ω–µ –¥–∞–≤–∞–ª–∏").

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (–°–¢–†–û–ì–û):
1. –¢–ï–ö–°–¢: –ü—Ä–æ—Å—Ç–æ –ø–∏—à–∏ —Ç–µ–∫—Å—Ç. –ë–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤.
2. –°–¢–ò–ö–ï–†–´: –ü–∏—à–∏ –°–¢–†–û–ì–û [STICKER] (–±–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è!), –µ—Å–ª–∏ —Ö–æ—á–µ—à—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å—Ç–∏–∫–µ—Ä.
3. –†–ï–ê–ö–¶–ò–ò: [REACT:emoji] ‚Äî —Ä–µ–¥–∫–æ.
"""

async def analyze_and_save_memory(db, chat_id, user_id, user_name, text):
    """–£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ–≤ (–æ–±–ª–µ–≥—á–µ–Ω–Ω–∞—è)"""
    if len(text) < 20: 
        return
    
    prompt = f"""Extract 1 key permanent fact about user '{user_name}' from: "{text}".
    If none, reply NO.
    Fact example: "–õ—é–±–∏—Ç –ø–∏—Ü—Ü—É", "–ñ–∏–≤–µ—Ç –≤ –í–∞–∫–µ", "–†–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–æ–≥–µ—Ä–æ–º".
    Reply in Russian, max 10 words.
    """
    
    try:
        # –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–∞–º—è—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±—ã—Å—Ç—Ä—É—é –º–æ–¥–µ–ª—å
        response = await client.chat.completions.create(
            model="google/gemini-2.0-flash-lite-preview-02-05:free", 
            messages=[{"role": "user", "content": prompt}],
            max_tokens=20,
            temperature=0.1
        )
        fact = response.choices[0].message.content.strip()
        if fact and "NO" not in fact.upper() and len(fact) > 5:
            bad_words = ["–ø—Ä–∏–≤–µ—Ç", "–±–æ—Ç", "–ø–æ–∫–∞", "–¥–µ–ª–∞", "–∫–∞–∫"]
            if not any(w in fact.lower() for w in bad_words):
                await db.add_fact(chat_id, user_id, user_name, fact)
    except Exception:
        pass 

def get_available_models_text():
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏"""
    models_list = ["ü§ñ **–î–æ—Å—Ç—É–ø–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:**\n"]
    
    models_list.append("\n**üëÅÔ∏è Vision (–¥–ª—è –∫–∞—Ä—Ç–∏–Ω–æ–∫):**")
    for key, model in sorted(VISION_MODELS.items(), key=lambda x: x[1]["priority"]):
        models_list.append(f"‚Ä¢ {model['display_name']} ‚Äî {model['description']}")
    
    models_list.append("\n**üìú –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è:**")
    for key, model in sorted(SUMMARIZATION_MODELS.items(), key=lambda x: x[1]["priority"]):
        models_list.append(f"‚Ä¢ {model['display_name']} ‚Äî {model['description']}")
    
    models_list.append("\n**‚ö° –ë—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã:**")
    for key, model in sorted(FAST_MODELS.items(), key=lambda x: x[1]["priority"]):
        models_list.append(f"‚Ä¢ {model['display_name']} ‚Äî {model['description']}")
    
    return "\n".join(models_list)

def clean_response(text):
    if not text: return ""
    text = str(text)
    # –ß–∏—Å—Ç–∫–∞ —Ç–µ–≥–æ–≤ –º—ã—à–ª–µ–Ω–∏—è
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    text = re.sub(r'^(Bot|System|Assistant|Yachejka|–Ø—á–µ–π–∫–∞):\s*', '', text.strip(), flags=re.IGNORECASE)
    return text.strip()

def is_refusal(text):
    text_lower = text.lower()
    triggers = ["language model", "–Ω–µ –º–æ–≥—É", "–Ω–µ—ç—Ç–∏—á–Ω–æ", "ai assistant", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"]
    return len(text) < 200 and any(t in text_lower for t in triggers)

def is_summary_query(text):
    triggers = ["—Å–∞–º–º–∞—Ä–∏", "summary", "—Å–≤–æ–¥–∫–∞", "–∏—Ç–æ–≥–∏", "–ø–µ—Ä–µ—Å–∫–∞–∂–∏", "–∫—Ä–∞—Ç–∫–æ", "–æ —á–µ–º —Ä–µ—á—å"]
    return text and any(t in text.lower() for t in triggers)

def is_event_query(text):
    triggers = ["–∫—É–¥–∞ —Å—Ö–æ–¥–∏—Ç—å", "–∞–Ω–æ–Ω—Å", "–≤—Å—Ç—Ä–µ—á–∞", "–ø–ª–∞–Ω—ã", "–∏–≤–µ–Ω—Ç", "—Å—Ö–æ–¥–∫–∞"]
    return text and any(t in text.lower() for t in triggers)

def get_system_prompt(memory_text="", query_type="chat"):
    prompt = BASE_SYSTEM_PROMPT
    
    if memory_text:
        prompt += f"\n[–ß–¢–û –¢–´ –ó–ù–ê–ï–®–¨ –ü–†–û –°–û–ë–ï–°–ï–î–ù–ò–ö–ê]: {memory_text}"
        
    if query_type == "summary":
        prompt += "\n–ó–ê–î–ê–ß–ê: –°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É –ø–µ—Ä–µ–ø–∏—Å–∫–∏. –ö—Ç–æ —á—Ç–æ –ø–∏—Å–∞–ª, –æ —á–µ–º —Å–ø–æ—Ä–∏–ª–∏. –ë–µ–∑ –≤–æ–¥—ã."
    elif query_type == "events":
        prompt += "\n–ó–ê–î–ê–ß–ê: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ø–∏—Å–æ–∫ –∞–Ω–æ–Ω—Å–æ–≤ –∏ –ø–æ–¥—Å–∫–∞–∂–∏, –∫—É–¥–∞ —Å—Ç–æ–∏—Ç —Å—Ö–æ–¥–∏—Ç—å."
    else:
        prompt += "\n–ù–ê–ü–û–ú–ò–ù–ê–ù–ò–ï: –ë—É–¥—å –∂–∏–≤–æ–π, –Ω–µ –¥—É—à–Ω–∏."
        
    return prompt

def select_model_queue(query_type, has_image):
    """–í—ã–±–æ—Ä –æ—á–µ—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞"""
    if has_image:
        # –î–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ–º vision-–º–æ–¥–µ–ª–∏
        return sorted(VISION_MODELS.values(), key=lambda x: x["priority"])
    elif query_type == "summary":
        # –î–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
        return sorted(SUMMARIZATION_MODELS.values(), key=lambda x: x["priority"])
    else:
        # –î–ª—è –æ–±—ã—á–Ω–æ–≥–æ —á–∞—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±—ã—Å—Ç—Ä—ã–µ –º–æ–¥–µ–ª–∏
        return sorted(FAST_MODELS.values(), key=lambda x: x["priority"])

async def generate_response(db, chat_id, thread_id, current_message, bot, image_data=None, user_id=None):
    limit_history = 50 if is_summary_query(current_message) else 8
    history_rows = await db.get_context(chat_id, thread_id, limit=limit_history)
    
    memory_text = ""
    if user_id:
        facts = await db.get_relevant_facts(chat_id, user_id)
        if facts:
            lines = [f"- {f['fact']}" for f in facts[:2]]
            memory_text = "; ".join(lines)

    found_events_text = ""
    query_type = "chat"
    
    if is_summary_query(current_message):
        query_type = "summary"
    elif is_event_query(current_message):
        query_type = "events"
        raw_events = await db.get_potential_announcements(chat_id, days=30, limit=3)
        if raw_events:
            lines = [f"- {e.get('content')[:150]}..." for e in raw_events]
            found_events_text = "\n".join(lines)

    system_prompt = get_system_prompt(memory_text, query_type)
    
    if query_type == "events" and found_events_text:
        system_prompt += f"\n\n[–ù–ê–ô–î–ï–ù–ù–´–ï –ê–ù–û–ù–°–´]:\n{found_events_text}"
    elif query_type == "events":
        system_prompt += "\n\n[–ê–ù–û–ù–°–´]: –ù–µ –Ω–∞–π–¥–µ–Ω–æ. –°–∫–∞–∂–∏, —á—Ç–æ –ø–æ–∫–∞ –≥–ª—É—Ö–æ."

    messages = [{"role": "system", "content": system_prompt}]
    
    for row in history_rows:
        role = "assistant" if row['role'] == "model" else "user"
        content = clean_response(row.get('content'))
        name = row.get('user_name', 'User')
        if content:
            msg = f"{name}: {content}" if role == "user" else content
            messages.append({"role": role, "content": msg})

    user_content = [{"type": "text", "text": current_message}]
    if image_data:
        try:
            buffered = io.BytesIO()
            image_data.save(buffered, format="JPEG", quality=80)
            b64 = base64.b64encode(buffered.getvalue()).decode("utf-8")
            user_content.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}})
        except: pass

    messages.append({"role": "user", "content": user_content})

    # –£–º–Ω—ã–π –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞—á–∏
    queue = select_model_queue(query_type, has_image=bool(image_data))

    for model_cfg in queue:
        try:
            logging.info(f"‚ö° Trying {model_cfg['name']}...")
            response = await client.chat.completions.create(
                model=model_cfg["name"],
                messages=messages,
                temperature=0.7,
                max_tokens=1000,
                extra_headers={"HTTP-Referer": "http://localhost:8080", "X-Title": "YachejkaBot"}
            )
            reply = clean_response(response.choices[0].message.content)
            
            if not reply or is_refusal(reply):
                logging.warning(f"‚ö†Ô∏è {model_cfg['display_name']} refused or empty")
                continue
                
            logging.info(f"‚úÖ Served by {model_cfg['display_name']}")
            return reply
            
        except Exception as e:
            logging.warning(f"‚ùå {model_cfg['display_name']} failed: {e}")
            continue

    return "–í—Å–µ –Ω–µ–π—Ä–æ–Ω–∫–∏ —Å–µ–π—á–∞—Å –æ—Ç–¥—ã—Ö–∞—é—Ç (–æ—à–∏–±–∫–∏ –¥–æ—Å—Ç—É–ø–∞). –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ."
