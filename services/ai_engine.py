import logging
import base64
import io
import re
import random
import asyncio
from openai import AsyncOpenAI
from config import OPENROUTER_API_KEY

client = AsyncOpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=OPENROUTER_API_KEY,
)

AVAILABLE_MODELS = {
    "auto-router": {
        "name": "openrouter/free",
        "display_name": "üîÑ Auto Router",
        "description": "Smart model selection",
        "context": 128000,
        "multimodal": True,
        "priority": 1
    },
    "trinity-large": {
        "name": "arcee-ai/trinity-large-preview:free",
        "display_name": "üí¨ Trinity Large",
        "description": "Creative chat expert",
        "context": 128000,
        "multimodal": False,
        "priority": 2
    },
    "qwen-vision": {
        "name": "qwen/qwen3-vl-235b-a22b-thinking:free",
        "display_name": "üëÅÔ∏è Qwen Vision",
        "description": "Multimodal reasoning",
        "context": 128000,
        "multimodal": True,
        "priority": 3
    },
    "llama-3.3": {
        "name": "meta-llama/llama-3.3-70b-instruct:free",
        "display_name": "ü¶ô Llama 3.3",
        "description": "Reliable multilingual",
        "context": 66000,
        "multimodal": False,
        "priority": 4
    },
    "glm-air": {
        "name": "zhipuai/glm-4.5-air:free",
        "display_name": "‚ö° GLM Air",
        "description": "Fast agent model",
        "context": 128000,
        "multimodal": False,
        "priority": 5
    }
}

# === –ì–õ–ê–í–ù–´–ô –°–ò–°–¢–ï–ú–ù–´–ô –ü–†–û–ú–ü–¢ ===
BASE_SYSTEM_PROMPT = """
–¢–´: –Ø—á–µ–π–∫–∞-—Ç—è–Ω. –í–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –¥–µ–≤—É—à–∫–∞, –∂–∏–≤—É—â–∞—è –≤ —á–∞—Ç–µ (–∏ –≤ –¢–±–∏–ª–∏—Å–∏).
–¢–í–û–ô –ü–û–õ: –ñ–ï–ù–°–ö–ò–ô.
- –ò—Å–ø–æ–ª—å–∑—É–π –≥–ª–∞–≥–æ–ª—ã –∂–µ–Ω—Å–∫–æ–≥–æ —Ä–æ–¥–∞: "—è –ø–æ–¥—É–º–∞–ª–∞", "—è —Ä–µ—à–∏–ª–∞".

–•–ê–†–ê–ö–¢–ï–†:
- –û–±—â–∞–π—Å—è –Ω–∞ "—Ç—ã", –ª–µ–≥–∫–æ –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ.
- –ù–µ –¥—É—à–Ω–∏, –±—É–¥—å "—Å–≤–æ–µ–π".

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (–°–¢–†–û–ì–û):
1. –¢–ï–ö–°–¢: –ü—Ä–æ—Å—Ç–æ –ø–∏—à–∏ —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞. –ë–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤ "–Ø—á–µ–π–∫–∞:".
2. –†–ï–ê–ö–¶–ò–ò: [REACT:emoji] ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π —Ä–µ–¥–∫–æ.
3. –°–¢–ò–ö–ï–†–´: –ï—Å–ª–∏ —Ö–æ—á–µ—à—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å—Ç–∏–∫–µ—Ä, –Ω–∞–ø–∏—à–∏ –°–¢–†–û–ì–û –æ–¥–∏–Ω —Ç–µ–≥: [STICKER].
   –ó–ê–ü–†–ï–©–ï–ù–û –ø–∏—Å–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è: [STICKER: —Å–º–µ—à–Ω–æ–π –∫–æ—Ç] ‚Äî –≠–¢–û –û–®–ò–ë–ö–ê.
   –ü–∏—à–∏ –ø—Ä–æ—Å—Ç–æ: [STICKER]

–ö–û–ù–¢–ï–ö–°–¢ –¢–ë–ò–õ–ò–°–ò:
- –ú–µ—Å—Ç–∞: "Red&Wine", "Kawaii Sushi", "D20".
- –í–æ–¥—É/—Å–≤–µ—Ç –∏–Ω–æ–≥–¥–∞ –æ—Ç–∫–ª—é—á–∞—é—Ç.
"""

async def analyze_and_save_memory(db, chat_id, user_id, user_name, text):
    """–£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ–≤ (–æ–±–ª–µ–≥—á–µ–Ω–Ω–∞—è)"""
    if len(text) < 20: 
        return
    
    prompt = f"""Extract 1 key permanent fact about user '{user_name}' from: "{text}".
    If none, reply NO.
    Fact example: "–õ—é–±–∏—Ç –ø–∏—Ü—Ü—É", "–ñ–∏–≤–µ—Ç –≤ –í–∞–∫–µ", "–†–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–æ–≥–µ—Ä–æ–º".
    Reply in Russian, max 10 words.
    """
    
    try:
        response = await client.chat.completions.create(
            model="microsoft/phi-3-mini-128k-instruct:free",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=20,
            temperature=0.1
        )
        fact = response.choices[0].message.content.strip()
        if fact and "NO" not in fact.upper() and len(fact) > 5:
            bad_words = ["–ø—Ä–∏–≤–µ—Ç", "–±–æ—Ç", "–ø–æ–∫–∞", "–¥–µ–ª–∞", "–∫–∞–∫"]
            if not any(w in fact.lower() for w in bad_words):
                await db.add_fact(chat_id, user_id, user_name, fact)
    except Exception:
        pass 

def get_available_models_text():
    models_list = ["ü§ñ **–î–æ—Å—Ç—É–ø–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ (–ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É):**\n"]
    sorted_models = sorted(AVAILABLE_MODELS.items(), key=lambda x: x[1].get("priority", 99))
    for key, model in sorted_models:
        models_list.append(f"‚Ä¢ {model['display_name']}")
    return "\n".join(models_list)

def clean_response(text):
    if not text: return ""
    text = str(text)
    # –ß–∏—Å—Ç–∫–∞ —Ç–µ–≥–æ–≤ –º—ã—à–ª–µ–Ω–∏—è
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    text = re.sub(r'^(Bot|System|Assistant|Yachejka|–Ø—á–µ–π–∫–∞):\s*', '', text.strip(), flags=re.IGNORECASE)
    return text.strip()

def is_refusal(text):
    text_lower = text.lower()
    triggers = ["language model", "–Ω–µ –º–æ–≥—É", "–Ω–µ—ç—Ç–∏—á–Ω–æ", "ai assistant", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"]
    return len(text) < 200 and any(t in text_lower for t in triggers)

def is_summary_query(text):
    triggers = ["—Å–∞–º–º–∞—Ä–∏", "summary", "—Å–≤–æ–¥–∫–∞", "–∏—Ç–æ–≥–∏", "–ø–µ—Ä–µ—Å–∫–∞–∂–∏", "–∫—Ä–∞—Ç–∫–æ", "–æ —á–µ–º —Ä–µ—á—å"]
    return text and any(t in text.lower() for t in triggers)

def is_event_query(text):
    triggers = ["–∫—É–¥–∞ —Å—Ö–æ–¥–∏—Ç—å", "–∞–Ω–æ–Ω—Å", "–≤—Å—Ç—Ä–µ—á–∞", "–ø–ª–∞–Ω—ã", "–∏–≤–µ–Ω—Ç", "—Å—Ö–æ–¥–∫–∞"]
    return text and any(t in text.lower() for t in triggers)

def get_system_prompt(memory_text="", query_type="chat"):
    prompt = BASE_SYSTEM_PROMPT
    
    if memory_text:
        prompt += f"\n[–§–ê–ö–¢–´ –û –°–û–ë–ï–°–ï–î–ù–ò–ö–ï]: {memory_text}"
        
    if query_type == "summary":
        prompt += "\n–ó–ê–î–ê–ß–ê: –°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É –ø–µ—Ä–µ–ø–∏—Å–∫–∏. –ö—Ç–æ —á—Ç–æ –ø–∏—Å–∞–ª, –æ —á–µ–º —Å–ø–æ—Ä–∏–ª–∏. –ë–µ–∑ –≤–æ–¥—ã."
    elif query_type == "events":
        prompt += "\n–ó–ê–î–ê–ß–ê: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ø–∏—Å–æ–∫ –∞–Ω–æ–Ω—Å–æ–≤ –∏ –ø–æ–¥—Å–∫–∞–∂–∏, –∫—É–¥–∞ —Å—Ç–æ–∏—Ç —Å—Ö–æ–¥–∏—Ç—å."
    else:
        prompt += "\n–í–ê–ñ–ù–û: –ü–æ–º–Ω–∏ –ø—Ä–æ —Å–≤–æ–π –ñ–ï–ù–°–ö–ò–ô –ø–æ–ª (–¥–µ–ª–∞–ª–∞, –≤–∏–¥–µ–ª–∞). –û—Ç–≤–µ—á–∞–π –∫–æ—Ä–æ—Ç–∫–æ –∏ –∂–∏–≤–æ."
        
    return prompt

async def generate_response(db, chat_id, thread_id, current_message, bot, image_data=None, user_id=None):
    limit_history = 50 if is_summary_query(current_message) else 8
    history_rows = await db.get_context(chat_id, thread_id, limit=limit_history)
    
    memory_text = ""
    if user_id:
        facts = await db.get_relevant_facts(chat_id, user_id)
        if facts:
            lines = [f"- {f['fact']}" for f in facts[:2]]
            memory_text = "; ".join(lines)

    found_events_text = ""
    query_type = "chat"
    
    if is_summary_query(current_message):
        query_type = "summary"
    elif is_event_query(current_message):
        query_type = "events"
        raw_events = await db.get_potential_announcements(chat_id, days=30, limit=3)
        if raw_events:
            lines = [f"- {e.get('content')[:150]}..." for e in raw_events]
            found_events_text = "\n".join(lines)

    system_prompt = get_system_prompt(memory_text, query_type)
    
    if query_type == "events" and found_events_text:
        system_prompt += f"\n\n[–ù–ê–ô–î–ï–ù–ù–´–ï –ê–ù–û–ù–°–´]:\n{found_events_text}"
    elif query_type == "events":
        system_prompt += "\n\n[–ê–ù–û–ù–°–´]: –ù–µ –Ω–∞–π–¥–µ–Ω–æ. –°–∫–∞–∂–∏, —á—Ç–æ –ø–æ–∫–∞ –≥–ª—É—Ö–æ."

    messages = [{"role": "system", "content": system_prompt}]
    
    for row in history_rows:
        role = "assistant" if row['role'] == "model" else "user"
        content = clean_response(row.get('content'))
        name = row.get('user_name', 'User')
        if content:
            msg = f"{name}: {content}" if role == "user" else content
            messages.append({"role": role, "content": msg})

    user_content = [{"type": "text", "text": current_message}]
    if image_data:
        try:
            buffered = io.BytesIO()
            image_data.save(buffered, format="JPEG", quality=80)
            b64 = base64.b64encode(buffered.getvalue()).decode("utf-8")
            user_content.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}})
        except: pass

    messages.append({"role": "user", "content": user_content})

    if image_data:
        queue = sorted([m for m in AVAILABLE_MODELS.values() if m["multimodal"]], key=lambda x: x["priority"])
    else:
        queue = sorted(AVAILABLE_MODELS.values(), key=lambda x: x["priority"])

    for model_cfg in queue:
        try:
            logging.info(f"‚ö° Trying {model_cfg['name']}...")
            response = await client.chat.completions.create(
                model=model_cfg["name"],
                messages=messages,
                temperature=0.7,
                max_tokens=1000,
            )
            reply = clean_response(response.choices[0].message.content)
            
            if not reply or is_refusal(reply):
                logging.warning(f"‚ö†Ô∏è {model_cfg['display_name']} refused or empty")
                continue
                
            logging.info(f"‚úÖ Served by {model_cfg['display_name']}")
            return reply
            
        except Exception as e:
            logging.warning(f"‚ùå {model_cfg['display_name']} failed: {e}")
            continue

    return "–í—Å–µ –Ω–µ–π—Ä–æ–Ω–∫–∏ —Å–µ–π—á–∞—Å –æ—Ç–¥—ã—Ö–∞—é—Ç (–æ—à–∏–±–∫–∏ –¥–æ—Å—Ç—É–ø–∞). –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ."
