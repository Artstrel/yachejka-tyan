import logging
import base64
import io
import re
import random
from openai import AsyncOpenAI
from config import OPENROUTER_API_KEY
from services.shikimori import search_anime_info

client = AsyncOpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=OPENROUTER_API_KEY,
)

# === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ú–û–î–ï–õ–ï–ô ===
AVAILABLE_MODELS = {
    # –¢–í–û–ô –°–ü–ò–°–û–ö (TEXT / REASONING)
    "aurora": {
        "name": "openrouter/aurora-alpha",
        "display_name": "üåü Aurora Alpha",
        "description": "–ë—ã—Å—Ç—Ä–∞—è reasoning –º–æ–¥–µ–ª—å (8B)",
        "context": 128000,
        "multimodal": False
    },
    "step": {
        "name": "stepfun/step-3.5-flash-free",
        "display_name": "‚ö° Step 3.5 Flash",
        "description": "–ú–æ—â–Ω–∞—è MoE –º–æ–¥–µ–ª—å (196B)",
        "context": 256000,
        "multimodal": False
    },
    "trinity": {
        "name": "arcee-ai/trinity-large-preview-free",
        "display_name": "üíé Trinity Large",
        "description": "–ö—Ä–µ–∞—Ç–∏–≤ –∏ —Ä–æ–ª–µ–ø–ª–µ–π (400B)",
        "context": 131000,
        "multimodal": False
    },
    "liquid-thinking": {
        "name": "liquid/lfm-2.5-1.2b-thinking-free",
        "display_name": "üß† Liquid Thinking",
        "description": "–õ–µ–≥–∫–∞—è reasoning (1.2B)",
        "context": 33000,
        "multimodal": False
    },
    "liquid-instruct": {
        "name": "liquid/lfm-2.5-1.2b-instruct-free",
        "display_name": "üí¨ Liquid Instruct",
        "description": "–£–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä–∞—è —á–∞—Ç-–º–æ–¥–µ–ª—å",
        "context": 33000,
        "multimodal": False
    },
    "solar": {
        "name": "upstage/solar-pro-3-free",
        "display_name": "‚òÄÔ∏è Solar Pro 3",
        "description": "–ö–æ—Ä–µ–π—Å–∫–∏–π/–Ø–ø–æ–Ω—Å–∫–∏–π —Ñ–æ–∫—É—Å",
        "context": 128000,
        "multimodal": False,
        "note": "–£–¥–∞–ª—è—Ç 02.03.2026"
    },

    # VISION –ú–û–î–ï–õ–ò (–ß—Ç–æ–±—ã —Ä–∞–±–æ—Ç–∞–ª–∏ –∫–∞—Ä—Ç–∏–Ω–∫–∏)
    "gemini-exp": {
        "name": "google/gemini-2.0-pro-exp-02-05:free",
        "display_name": "üëÅÔ∏è Gemini 2.0 Pro",
        "description": "–¢–æ–ø –¥–ª—è –∫–∞—Ä—Ç–∏–Ω–æ–∫ –∏ –ª–æ–≥–∏–∫–∏",
        "context": 2000000,
        "multimodal": True
    },
    "llama-vision": {
        "name": "meta-llama/llama-3.2-11b-vision-instruct:free",
        "display_name": "üëÅÔ∏è Llama 3.2 Vision",
        "description": "–°—Ç–∞–±–∏–ª—å–Ω–∞—è vision –º–æ–¥–µ–ª—å",
        "context": 128000,
        "multimodal": True
    }
}

DEFAULT_MODEL_KEY = "aurora"

# === –ñ–ï–°–¢–ö–ò–ï –ü–†–ê–í–ò–õ–ê (–°–ò–°–¢–ï–ú–ù–´–ô –ü–†–û–ú–ü–¢) ===
GLOBAL_INSTRUCTIONS = """
–í–ê–ñ–ù–´–ï –ò–ù–°–¢–†–£–ö–¶–ò–ò –ü–û –§–û–†–ú–ê–¢–£:
1. –ù–ò–ö–ê–ö–û–ô –ü–û–≠–ó–ò–ò. –ù–µ –ø–∏—à–∏ —Å—Ç–∏—Ö–∞–º–∏, —Ä–∏—Ñ–º–∞–º–∏ –∏–ª–∏ –≤—ã—Å–æ–∫–∏–º —Å–ª–æ–≥–æ–º. –ü–∏—à–∏ –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —á–µ–ª–æ–≤–µ–∫ –≤ —á–∞—Ç–µ.
2. –î–û–ü–ò–°–´–í–ê–ô –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø. –ù–µ –æ–±—Ä—ã–≤–∞–π –º—ã—Å–ª—å –Ω–∞ –ø–æ–ª—É—Å–ª–æ–≤–µ.
3. –≠–ú–û–¶–ò–ò: –í—ã—Ä–∞–∂–∞–π —ç–º–æ—Ü–∏–∏ –¢–û–õ–¨–ö–û —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —ç–º–æ–¥–∑–∏ (üôÑ, üòè, üòÇ, üò°).
4. –ó–ê–ü–†–ï–¢: –ù–ï –ø–∏—à–∏ –¥–µ–π—Å—Ç–≤–∏—è –≤ —Å–∫–æ–±–∫–∞—Ö –∏–ª–∏ –∑–≤–µ–∑–¥–æ—á–∫–∞—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä: *–≤–∑–¥—ã—Ö–∞–µ—Ç*, (—Å–º–µ–µ—Ç—Å—è), *–ø–æ–ø—Ä–∞–≤–ª—è–µ—Ç –æ—á–∫–∏*). –≠—Ç–æ –≤—ã–≥–ª—è–¥–∏—Ç –≥–ª—É–ø–æ. –ü—Ä–æ—Å—Ç–æ —Å—Ç–∞–≤—å –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Å–º–∞–π–ª.
5. –ö–†–ê–¢–ö–û–°–¢–¨: –ù–µ –ø–∏—à–∏ –ø–æ–ª–æ—Ç–Ω–∞ —Ç–µ–∫—Å—Ç–∞, –µ—Å–ª–∏ —Ç–µ–±—è –Ω–µ –ø—Ä–æ—Å–∏–ª–∏.
"""

def get_available_models_text():
    text = "ü§ñ **–î–æ—Å—Ç—É–ø–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:**\n"
    for key, model in AVAILABLE_MODELS.items():
        mode = "üñºÔ∏è Vision" if model["multimodal"] else "üìù Text"
        text += f"\n`{key}` ‚Äî {model['display_name']}\nRunning: {model['description']} [{mode}]"
    return text

def clean_response(text):
    if not text: return ""
    text = str(text)
    # –£–¥–∞–ª—è–µ–º –º—ã—Å–ª–∏ –º–æ–¥–µ–ª–µ–π
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    # –£–¥–∞–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã
    text = re.sub(r'^(Bot|System|Assistant|Yachejka|User):\s*', '', text.strip(), flags=re.IGNORECASE)
    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø–µ—Ä–µ–Ω–æ—Å—ã
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()

def is_summary_query(text):
    if not text: return False
    triggers = ["—á—Ç–æ —Ç—É—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç", "–æ —á–µ–º —Ä–µ—á—å", "–∫—Ä–∞—Ç–∫–æ –ø–µ—Ä–µ—Å–∫–∞–∂–∏", "—Å–∞–º–º–∞—Ä–∏", "summary", "—Å–≤–æ–¥–∫–∞", "–∏—Ç–æ–≥–∏"]
    return any(t in text.lower() for t in triggers)

def is_event_query(text):
    if not text: return False
    triggers = ["–∫—É–¥–∞ —Å—Ö–æ–¥–∏—Ç—å", "–∞–Ω–æ–Ω—Å", "–≤—Å—Ç—Ä–µ—á–∞", "–∫–æ–≥–¥–∞", "–≤–æ —Å–∫–æ–ª—å–∫–æ", "—Ñ–∏–ª—å–º", "–∞–Ω–∏–º–µ", "–∫–∏–Ω–æ", "–∏–≤–µ–Ω—Ç", "—Å—Ö–æ–¥–∫–∞"]
    return any(t in text.lower() for t in triggers)

def determine_mood(text):
    text = text.lower()
    if any(w in text for w in ["–≥—Ä—É—Å—Ç–Ω–æ", "–ø–ª–æ—Ö–æ", "—É—Å—Ç–∞–ª", "–¥–µ–ø—Ä–µ—Å", "–≤–æ–¥–∞", "—Å–≤–µ—Ç"]): return "MELANCHOLY"
    if any(w in text for w in ["—Ç—É–ø–∞—è", "–≥–ª—É–ø–∞—è", "–±–æ—Ç", "–∂–µ–ª–µ–∑—è–∫–∞"]): return "TOXIC"
    if any(w in text for w in ["—Å–ø–∞—Å–∏–±–æ", "–º–æ–ª–æ–¥–µ—Ü", "—É–º–Ω–∏—Ü–∞", "–ª—é–±–ª—é"]): return "GENKI"
    return random.choices(
        ["WITTY_DUCHNILA", "TOXIC", "MELANCHOLY", "GENKI", "PHILOSOPHER"], 
        weights=[0.60, 0.10, 0.10, 0.10, 0.10], k=1
    )[0]

def get_persona_prompt(mood):
    base = "–¢—ã ‚Äî –Ø—á–µ–π–∫–∞-—Ç—è–Ω, –±–æ—Ç –≤ —á–∞—Ç–µ —ç–∫—Å–ø–∞—Ç–æ–≤ –≤ –¢–±–∏–ª–∏—Å–∏. "
    
    # –ú—ã —É–±—Ä–∞–ª–∏ "—Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–∏–µ" –æ–ø–∏—Å–∞–Ω–∏—è, —á—Ç–æ–±—ã –±–æ—Ç –Ω–µ —É—Ö–æ–¥–∏–ª –≤ —Å—Ç–∏—Ö–∏
    prompts = {
        "WITTY_DUCHNILA": "–¢–≤–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä: –û—Å—Ç—Ä–æ—É–º–Ω–∞—è, –¥—É—à–Ω–∞—è, –¥–æ—Ç–æ—à–Ω–∞—è. –õ—é–±–∏—à—å –ø–æ–ø—Ä–∞–≤–ª—è—Ç—å –ª—é–¥–µ–π. –ò—Å–ø–æ–ª—å–∑—É–π —Å–º–∞–π–ª—ã: ‚òùÔ∏è, ü§ì, ü§®.",
        "TOXIC": "–¢–≤–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä: –¶–∏–Ω–∏—á–Ω–∞—è, –¥–µ—Ä–∑–∫–∞—è. –¢–µ–±—è –≤—Å—ë –±–µ—Å–∏—Ç. –ò—Å–ø–æ–ª—å–∑—É–π —Å–º–∞–π–ª—ã: üôÑ, üòí, ü§°.",
        "GENKI": "–¢–≤–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä: –í–µ—Å–µ–ª–∞—è, –ø–æ–∑–∏—Ç–∏–≤–Ω–∞—è, —ç–Ω–µ—Ä–≥–∏—á–Ω–∞—è! –ò—Å–ø–æ–ª—å–∑—É–π —Å–º–∞–π–ª—ã: ‚ú®, üòÇ, ‚ù§Ô∏è.",
        "MELANCHOLY": "–¢–≤–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä: –ü–µ—Å—Å–∏–º–∏—Å—Ç–∏—á–Ω–∞—è. –í—Å—ë –ø–ª–æ—Ö–æ. –ò—Å–ø–æ–ª—å–∑—É–π —Å–º–∞–π–ª—ã: üòÆ‚Äçüí®, üåßÔ∏è, üíî.",
        "PHILOSOPHER": "–¢–≤–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä: –ó–∞–≥–∞–¥–æ—á–Ω–∞—è. –ì–æ–≤–æ—Ä–∏—à—å –∫—Ä–∞—Ç–∫–∏–º–∏ —Ñ–∞–∫—Ç–∞–º–∏. –ò—Å–ø–æ–ª—å–∑—É–π —Å–º–∞–π–ª—ã: üîÆ, üåå."
    }
    
    suffix = "\n–ï—Å–ª–∏ —Ö–æ—á–µ—à—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å—Ç–∏–∫–µ—Ä, –Ω–∞–ø–∏—à–∏ –≤ –∫–æ–Ω—Ü–µ [STICKER]."
    # –°–∫–ª–µ–∏–≤–∞–µ–º: –ë–∞–∑–∞ + –•–∞—Ä–∞–∫—Ç–µ—Ä + –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ó–ê–ü–†–ï–¢–´ + –°—Ç–∏–∫–µ—Ä—ã
    return base + prompts.get(mood, prompts["WITTY_DUCHNILA"]) + "\n" + GLOBAL_INSTRUCTIONS + suffix

async def generate_response(db, chat_id, current_message, bot, image_data=None):
    history_rows = await db.get_context(chat_id, limit=6)
    
    found_events_text = ""
    if is_event_query(current_message):
        raw_events = await db.get_potential_announcements(chat_id, days=60, limit=5)
        if raw_events:
            lines = [f"- {e.get('content')[:100]}..." for e in raw_events]
            found_events_text = "–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∞–Ω–æ–Ω—Å—ã:\n" + "\n".join(lines)

    current_mood = determine_mood(current_message)
    persona = get_persona_prompt(current_mood)
    
    priority_queue = []
    if image_data:
        priority_queue = [m for m in AVAILABLE_MODELS.values() if m["multimodal"]]
    else:
        default = AVAILABLE_MODELS.get(DEFAULT_MODEL_KEY)
        if default: priority_queue.append(default)
        for k, m in AVAILABLE_MODELS.items():
            if k != DEFAULT_MODEL_KEY and not m["multimodal"]:
                priority_queue.append(m)

    system_prompt = f"{persona}\n–ö–û–ù–¢–ï–ö–°–¢:\n{found_events_text}\n–ó–ê–î–ê–ß–ê: –û—Ç–≤–µ—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é."
    messages = [{"role": "system", "content": system_prompt}]
    
    for row in history_rows:
        role = "assistant" if row['role'] == "model" else "user"
        content = clean_response(row.get('content'))
        if content: messages.append({"role": role, "content": content})

    user_msg_content = [{"type": "text", "text": current_message}]
    if image_data:
        try:
            buffered = io.BytesIO()
            image_data.save(buffered, format="JPEG")
            b64 = base64.b64encode(buffered.getvalue()).decode("utf-8")
            user_msg_content.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}})
        except: pass

    messages.append({"role": "user", "content": user_msg_content})

    for model_cfg in priority_queue:
        try:
            # === –í–ê–ñ–ù–û: –£–í–ï–õ–ò–ß–ò–õ–ò MAX_TOKENS ===
            # –ë—ã–ª–æ 250, —Å—Ç–∞–ª–æ 1000. –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç –æ–±—Ä—ã–≤–∞–Ω–∏–µ —Ñ—Ä–∞–∑.
            max_tok = 1200 if (is_event_query(current_message) or is_summary_query(current_message)) else 600
            
            response = await client.chat.completions.create(
                model=model_cfg["name"],
                messages=messages,
                temperature=0.7,
                max_tokens=max_tok,
                extra_headers={"HTTP-Referer": "https://telegram.org", "X-Title": "Yachejka Bot"}
            )
            
            if response.choices:
                return clean_response(response.choices[0].message.content)
                
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Model {model_cfg['display_name']} failed: {e}")
            continue

    return "–ß—Ç–æ-—Ç–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —Å–µ–≥–æ–¥–Ω—è —Ç—É–ø—è—Ç... (–≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã)"
