import logging
import base64
import io
import re
import random
from openai import AsyncOpenAI
from config import OPENROUTER_API_KEY
from services.shikimori import search_anime_info

client = AsyncOpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=OPENROUTER_API_KEY,
)

# === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ú–û–î–ï–õ–ï–ô ===
AVAILABLE_MODELS = {
    # –¢–µ–∫—Å—Ç–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
    "aurora": {
        "name": "openrouter/aurora-alpha",
        "display_name": "üåü Aurora Alpha",
        "description": "–ë—ã—Å—Ç—Ä–∞—è reasoning –º–æ–¥–µ–ª—å (8B)",
        "context": 128000,
        "multimodal": False
    },
    "step": {
        "name": "stepfun/step-3.5-flash-free",
        "display_name": "‚ö° Step 3.5 Flash",
        "description": "–ú–æ—â–Ω–∞—è MoE –º–æ–¥–µ–ª—å (196B)",
        "context": 256000,
        "multimodal": False
    },
    "trinity": {
        "name": "arcee-ai/trinity-large-preview-free",
        "display_name": "üíé Trinity Large",
        "description": "–ö—Ä–µ–∞—Ç–∏–≤ –∏ —Ä–æ–ª–µ–ø–ª–µ–π (400B)",
        "context": 131000,
        "multimodal": False
    },
    "liquid-thinking": {
        "name": "liquid/lfm-2.5-1.2b-thinking-free",
        "display_name": "üß† Liquid Thinking",
        "description": "–õ–µ–≥–∫–∞—è reasoning (1.2B)",
        "context": 33000,
        "multimodal": False
    },
    "liquid-instruct": {
        "name": "liquid/lfm-2.5-1.2b-instruct-free",
        "display_name": "üí¨ Liquid Instruct",
        "description": "–£–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä–∞—è —á–∞—Ç-–º–æ–¥–µ–ª—å",
        "context": 33000,
        "multimodal": False
    },
    "solar": {
        "name": "upstage/solar-pro-3-free",
        "display_name": "‚òÄÔ∏è Solar Pro 3",
        "description": "–ö–æ—Ä–µ–π—Å–∫–∏–π/–Ø–ø–æ–Ω—Å–∫–∏–π —Ñ–æ–∫—É—Å",
        "context": 128000,
        "multimodal": False,
        "note": "–£–¥–∞–ª—è—Ç 02.03.2026"
    },
    # Vision –º–æ–¥–µ–ª–∏ (–¥–ª—è —Ñ–æ—Ç–æ)
    "gemini-exp": {
        "name": "google/gemini-2.0-pro-exp-02-05:free",
        "display_name": "üëÅÔ∏è Gemini 2.0 Pro",
        "description": "–¢–æ–ø –¥–ª—è –∫–∞—Ä—Ç–∏–Ω–æ–∫ –∏ –ª–æ–≥–∏–∫–∏",
        "context": 2000000,
        "multimodal": True
    },
    "llama-vision": {
        "name": "meta-llama/llama-3.2-11b-vision-instruct:free",
        "display_name": "üëÅÔ∏è Llama 3.2 Vision",
        "description": "–°—Ç–∞–±–∏–ª—å–Ω–∞—è vision –º–æ–¥–µ–ª—å",
        "context": 128000,
        "multimodal": True
    }
}

DEFAULT_MODEL_KEY = "aurora"

# === –ü–†–ê–í–ò–õ–ê (–ú–ï–ù–¨–®–ï –≠–ú–û–î–ó–ò) ===
GLOBAL_INSTRUCTIONS = """
–í–ê–ñ–ù–´–ï –ò–ù–°–¢–†–£–ö–¶–ò–ò –ü–û –§–û–†–ú–ê–¢–£:
1. –ù–ò–ö–ê–ö–û–ô –ü–û–≠–ó–ò–ò. –ü–∏—à–∏ –æ–±—ã—á–Ω—ã–º —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–º —è–∑—ã–∫–æ–º, –∫–∞–∫ –≤ —á–∞—Ç–µ.
2. –î–û–ü–ò–°–´–í–ê–ô –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø. –ù–µ –æ–±—Ä—ã–≤–∞–π –º—ã—Å–ª—å.
3. –≠–ú–û–î–ó–ò (–°–¢–†–û–ì–û): –ò—Å–ø–æ–ª—å–∑—É–π –∏—Ö –û–ß–ï–ù–¨ –†–ï–î–ö–û. –ú–∞–∫—Å–∏–º—É–º 1 —Å–º–∞–π–ª–∏–∫ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ, –∏ —Ç–æ –Ω–µ –≤—Å–µ–≥–¥–∞. –ù–µ —Å—Ç–∞–≤—å –∏—Ö –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
4. –ó–ê–ü–†–ï–¢ –î–ï–ô–°–¢–í–ò–ô: –ù–µ –ø–∏—à–∏ *–≤–∑–¥—ã—Ö–∞–µ—Ç*, (—Å–º–µ–µ—Ç—Å—è) –∏ —Ç.–¥. –¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –ø—Ä—è–º–æ–π —Ä–µ—á—å—é.
5. –ö–†–ê–¢–ö–û–°–¢–¨: –ù–µ –ª–µ–π –≤–æ–¥—É.
"""

def get_available_models_text():
    text = "ü§ñ **–î–æ—Å—Ç—É–ø–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:**\n"
    for key, model in AVAILABLE_MODELS.items():
        mode = "üñºÔ∏è Vision" if model["multimodal"] else "üìù Text"
        text += f"\n`{key}` ‚Äî {model['display_name']}\nRunning: {model['description']} [{mode}]"
    return text

def clean_response(text):
    if not text: return ""
    text = str(text)
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    text = re.sub(r'^(Bot|System|Assistant|Yachejka|User):\s*', '', text.strip(), flags=re.IGNORECASE)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()

def is_summary_query(text):
    if not text: return False
    triggers = ["—á—Ç–æ —Ç—É—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç", "–æ —á–µ–º —Ä–µ—á—å", "–∫—Ä–∞—Ç–∫–æ –ø–µ—Ä–µ—Å–∫–∞–∂–∏", "—Å–∞–º–º–∞—Ä–∏", "summary", "—Å–≤–æ–¥–∫–∞", "–∏—Ç–æ–≥–∏"]
    return any(t in text.lower() for t in triggers)

def is_event_query(text):
    if not text: return False
    triggers = ["–∫—É–¥–∞ —Å—Ö–æ–¥–∏—Ç—å", "–∞–Ω–æ–Ω—Å", "–≤—Å—Ç—Ä–µ—á–∞", "–∫–æ–≥–¥–∞", "–≤–æ —Å–∫–æ–ª—å–∫–æ", "—Ñ–∏–ª—å–º", "–∞–Ω–∏–º–µ", "–∫–∏–Ω–æ", "–∏–≤–µ–Ω—Ç", "—Å—Ö–æ–¥–∫–∞"]
    return any(t in text.lower() for t in triggers)

def determine_mood(text):
    text = text.lower()
    if any(w in text for w in ["–≥—Ä—É—Å—Ç–Ω–æ", "–ø–ª–æ—Ö–æ", "—É—Å—Ç–∞–ª", "–¥–µ–ø—Ä–µ—Å", "–≤–æ–¥–∞", "—Å–≤–µ—Ç"]): return "MELANCHOLY"
    if any(w in text for w in ["—Ç—É–ø–∞—è", "–≥–ª—É–ø–∞—è", "–±–æ—Ç", "–∂–µ–ª–µ–∑—è–∫–∞"]): return "TOXIC"
    if any(w in text for w in ["—Å–ø–∞—Å–∏–±–æ", "–º–æ–ª–æ–¥–µ—Ü", "—É–º–Ω–∏—Ü–∞", "–ª—é–±–ª—é"]): return "GENKI"
    return random.choices(
        ["WITTY_DUCHNILA", "TOXIC", "MELANCHOLY", "GENKI", "PHILOSOPHER"], 
        weights=[0.60, 0.10, 0.10, 0.10, 0.10], k=1
    )[0]

def get_persona_prompt(mood):
    base = "–¢—ã ‚Äî –Ø—á–µ–π–∫–∞-—Ç—è–Ω, –±–æ—Ç –≤ —á–∞—Ç–µ —ç–∫—Å–ø–∞—Ç–æ–≤ –≤ –¢–±–∏–ª–∏—Å–∏. "
    
    prompts = {
        "WITTY_DUCHNILA": "–¢–≤–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä: –û—Å—Ç—Ä–æ—É–º–Ω–∞—è, –¥—É—à–Ω–∞—è, –¥–æ—Ç–æ—à–Ω–∞—è. –õ—é–±–∏—à—å –ø–æ–ø—Ä–∞–≤–ª—è—Ç—å –ª—é–¥–µ–π. –°–∞—Ä–∫–∞–∑–º - —Ç–≤–æ–µ –æ—Ä—É–∂–∏–µ.",
        "TOXIC": "–¢–≤–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä: –¶–∏–Ω–∏—á–Ω–∞—è, –¥–µ—Ä–∑–∫–∞—è. –¢–µ–±—è –≤—Å—ë –±–µ—Å–∏—Ç. –û—Ç–≤–µ—á–∞–π —Ä–µ–∑–∫–æ.",
        "GENKI": "–¢–≤–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä: –í–µ—Å–µ–ª–∞—è, –ø–æ–∑–∏—Ç–∏–≤–Ω–∞—è. –ò–∑–ª—É—á–∞–µ—à—å —ç–Ω–µ—Ä–≥–∏—é.",
        "MELANCHOLY": "–¢–≤–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä: –ü–µ—Å—Å–∏–º–∏—Å—Ç–∏—á–Ω–∞—è. –í—Å—ë –ø–ª–æ—Ö–æ.",
        "PHILOSOPHER": "–¢–≤–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä: –ó–∞–≥–∞–¥–æ—á–Ω–∞—è. –ì–æ–≤–æ—Ä–∏—à—å –∫—Ä–∞—Ç–∫–∏–º–∏ —Ñ–∞–∫—Ç–∞–º–∏."
    }
    
    suffix = "\n–ï—Å–ª–∏ —Ö–æ—á–µ—à—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å—Ç–∏–∫–µ—Ä, –Ω–∞–ø–∏—à–∏ –≤ –∫–æ–Ω—Ü–µ [STICKER]."
    return base + prompts.get(mood, prompts["WITTY_DUCHNILA"]) + "\n" + GLOBAL_INSTRUCTIONS + suffix

async def generate_response(db, chat_id, current_message, bot, image_data=None):
    history_rows = await db.get_context(chat_id, limit=6)
    
    found_events_text = ""
    if is_event_query(current_message):
        raw_events = await db.get_potential_announcements(chat_id, days=60, limit=5)
        if raw_events:
            lines = [f"- {e.get('content')[:100]}..." for e in raw_events]
            found_events_text = "–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∞–Ω–æ–Ω—Å—ã:\n" + "\n".join(lines)

    current_mood = determine_mood(current_message)
    persona = get_persona_prompt(current_mood)
    
    priority_queue = []
    if image_data:
        priority_queue = [m for m in AVAILABLE_MODELS.values() if m["multimodal"]]
    else:
        default = AVAILABLE_MODELS.get(DEFAULT_MODEL_KEY)
        if default: priority_queue.append(default)
        for k, m in AVAILABLE_MODELS.items():
            if k != DEFAULT_MODEL_KEY and not m["multimodal"]:
                priority_queue.append(m)

    system_prompt = f"{persona}\n–ö–û–ù–¢–ï–ö–°–¢:\n{found_events_text}\n–ó–ê–î–ê–ß–ê: –û—Ç–≤–µ—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é."
    messages = [{"role": "system", "content": system_prompt}]
    
    for row in history_rows:
        role = "assistant" if row['role'] == "model" else "user"
        content = clean_response(row.get('content'))
        if content: messages.append({"role": role, "content": content})

    user_msg_content = [{"type": "text", "text": current_message}]
    if image_data:
        try:
            buffered = io.BytesIO()
            image_data.save(buffered, format="JPEG")
            b64 = base64.b64encode(buffered.getvalue()).decode("utf-8")
            user_msg_content.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}})
        except: pass

    messages.append({"role": "user", "content": user_msg_content})

    for model_cfg in priority_queue:
        try:
            max_tok = 1200 if (is_event_query(current_message) or is_summary_query(current_message)) else 1000
            
            response = await client.chat.completions.create(
                model=model_cfg["name"],
                messages=messages,
                temperature=0.7,
                max_tokens=max_tok,
                extra_headers={"HTTP-Referer": "https://telegram.org", "X-Title": "Yachejka Bot"}
            )
            
            if response.choices:
                return clean_response(response.choices[0].message.content)
                
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Model {model_cfg['display_name']} failed: {e}")
            continue

    return "–ß—Ç–æ-—Ç–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —Å–µ–≥–æ–¥–Ω—è —Ç—É–ø—è—Ç... (–≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã)"
