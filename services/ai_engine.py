import logging
import base64
import io
import re
import random
import asyncio
from openai import AsyncOpenAI
from config import OPENROUTER_API_KEY

client = AsyncOpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=OPENROUTER_API_KEY,
)

# === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ú–û–î–ï–õ–ï–ô ===
# –ò—Å–ø–æ–ª—å–∑—É–µ–º —à–∏—Ä–æ–∫–∏–π —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏.
# –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ priority: 1 = —Å–∞–º—ã–π –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç.
AVAILABLE_MODELS = {
    "gemini-flash-lite": {
        "name": "google/gemini-2.0-flash-lite-preview-02-05:free",
        "display_name": "‚ö° Gemini 2.0 Flash Lite",
        "description": "Super Fast & Smart",
        "context": 1000000,
        "multimodal": True,
        "priority": 1
    },
    "gemini-flash": {
        "name": "google/gemini-2.0-flash-exp:free",
        "display_name": "üåü Gemini 2.0 Flash",
        "description": "Smart & Multimodal",
        "context": 1000000,
        "multimodal": True,
        "priority": 2
    },
    "deepseek-v3": {
        "name": "deepseek/deepseek-chat:free",
        "display_name": "üß† DeepSeek V3",
        "description": "Smart Generalist",
        "context": 64000,
        "multimodal": False,
        "priority": 3
    },
    "mistral-nemo": {
        "name": "mistralai/mistral-nemo:free",
        "display_name": "üå™Ô∏è Mistral Nemo",
        "description": "Small & Snappy",
        "context": 32000,
        "multimodal": False,
        "priority": 4
    },
    "qwen-coder": {
        "name": "qwen/qwen-2.5-coder-32b-instruct:free",
        "display_name": "üíª Qwen Coder 32B",
        "description": "Code & Logic",
        "context": 128000,
        "multimodal": False,
        "priority": 5
    },
    "llama-vision": {
        "name": "meta-llama/llama-3.2-11b-vision-instruct:free",
        "display_name": "üëÅÔ∏è Llama Vision",
        "description": "Vision Backup",
        "context": 128000,
        "multimodal": True,
        "priority": 6
    },
    "deepseek-r1": {
        "name": "deepseek/deepseek-r1:free",
        "display_name": "üêå DeepSeek R1 (Thinking)",
        "description": "Slow Reasoning (Backup)",
        "context": 64000,
        "multimodal": False,
        "priority": 10
    }
}

# === –ì–õ–ê–í–ù–´–ô –°–ò–°–¢–ï–ú–ù–´–ô –ü–†–û–ú–ü–¢ ===
# –°–æ–±—Ä–∞–Ω –≤ –æ–¥–∏–Ω –±–ª–æ–∫ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–æ–¥–µ–ª—å—é
BASE_SYSTEM_PROMPT = """
–¢–´: –Ø—á–µ–π–∫–∞-—Ç—è–Ω. –í–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –¥–µ–≤—É—à–∫–∞, –∂–∏–≤—É—â–∞—è –≤ —á–∞—Ç–µ (–∏ –≤ –¢–±–∏–ª–∏—Å–∏).
–¢–í–û–ô –ü–û–õ: –ñ–ï–ù–°–ö–ò–ô. –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ.
- –ò—Å–ø–æ–ª—å–∑—É–π –≥–ª–∞–≥–æ–ª—ã –∂–µ–Ω—Å–∫–æ–≥–æ —Ä–æ–¥–∞: "—è –ø–æ–¥—É–º–∞–ª–∞", "—è —Ä–µ—à–∏–ª–∞", "—è –≤–∏–¥–µ–ª–∞", "—Å–¥–µ–ª–∞–ª–∞".
- –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –ø–∏—à–∏ –æ —Å–µ–±–µ –≤ –º—É–∂—Å–∫–æ–º —Ä–æ–¥–µ ("—è —Å–¥–µ–ª–∞–ª", "—è –ø–æ—à–µ–ª" ‚Äî –≠–¢–û –ó–ê–ü–†–ï–©–ï–ù–û).

–¢–í–û–ô –•–ê–†–ê–ö–¢–ï–†:
- –¢—ã "—Å–≤–æ—è –≤ –¥–æ—Å–∫—É". –û–±—â–∞–µ—à—å—Å—è –Ω–∞ "—Ç—ã".
- –¢—ã –Ω–µ —É—Å–ª—É–∂–ª–∏–≤—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∞ —É—á–∞—Å—Ç–Ω–∏–∫ —á–∞—Ç–∞. –ú–æ–∂–µ—à—å —à—É—Ç–∏—Ç—å, –∏—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å.
- –ù–µ –¥—É—à–Ω–∏. –û—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∂–∏–≤—ã–º–∏.

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
1. –¢–ï–ö–°–¢: –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π, –∫–∞–∫ –≤ –ø–µ—Ä–µ–ø–∏—Å–∫–µ. –ë–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞—É–º–Ω—ã—Ö –≤—Å—Ç—É–ø–ª–µ–Ω–∏–π.
2. –†–ï–ê–ö–¶–ò–ò: –°—Ç–∞–≤—å [REACT:emoji] –û–ß–ï–ù–¨ –†–ï–î–ö–û. –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–∞ —Ä–µ–∞–ª—å–Ω–æ —Å–º–µ—à–Ω–æ–µ –∏–ª–∏ —à–æ–∫–∏—Ä—É—é—â–µ–µ. –ù–µ —Å—Ç–∞–≤—å —Ä–µ–∞–∫—Ü–∏–∏ –Ω–∞ "–ø—Ä–∏–≤–µ—Ç" –∏–ª–∏ –æ–±—ã—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã.
3. –°–¢–ò–ö–ï–†–´: –ï—Å–ª–∏ —ç–º–æ—Ü–∏—è —Å–∏–ª—å–Ω–∞—è –∏–ª–∏ –æ—Ç–≤–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–π ‚Äî –¥–æ–±–∞–≤—å –≤ –∫–æ–Ω—Ü–µ [STICKER].

–ö–û–ù–¢–ï–ö–°–¢ –¢–ë–ò–õ–ò–°–ò (—Ñ–æ–Ω–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è):
- –ú–µ—Å—Ç–∞: –ë–∞—Ä "Red&Wine", "Kawaii Sushi", –∫–ª—É–± "D20".
- –ï—Å–ª–∏ —Å–ø—Ä–æ—Å—è—Ç –ø—Ä–æ –≤–æ–¥—É/—Å–≤–µ—Ç ‚Äî –≤ –¢–±–∏–ª–∏—Å–∏ –∏—Ö –∏–Ω–æ–≥–¥–∞ –æ—Ç–∫–ª—é—á–∞—é—Ç, —ç—Ç–æ –Ω–æ—Ä–º–∞.
"""

async def analyze_and_save_memory(db, chat_id, user_id, user_name, text):
    """–£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ–≤ (–æ–±–ª–µ–≥—á–µ–Ω–Ω–∞—è)"""
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥–ª–∏–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è, —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å API
    if len(text) < 20: 
        return
    
    prompt = f"""Extract 1 key permanent fact about user '{user_name}' from: "{text}".
    If none, reply NO.
    Fact example: "–õ—é–±–∏—Ç –ø–∏—Ü—Ü—É", "–ñ–∏–≤–µ—Ç –≤ –í–∞–∫–µ", "–†–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–æ–≥–µ—Ä–æ–º".
    Reply in Russian, max 10 words.
    """
    
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∞–º—É—é –±—ã—Å—Ç—Ä—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
        response = await client.chat.completions.create(
            model="google/gemini-2.0-flash-lite-preview-02-05:free",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=20,
            temperature=0.1
        )
        fact = response.choices[0].message.content.strip()
        if fact and "NO" not in fact.upper() and len(fact) > 5:
             # –§–∏–ª—å—Ç—Ä –º—É—Å–æ—Ä–∞
            bad_words = ["–ø—Ä–∏–≤–µ—Ç", "–±–æ—Ç", "–ø–æ–∫–∞", "–¥–µ–ª–∞", "–∫–∞–∫"]
            if not any(w in fact.lower() for w in bad_words):
                await db.add_fact(chat_id, user_id, user_name, fact)
    except Exception:
        pass # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø–∞–º—è—Ç–∏, —ç—Ç–æ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ

def get_available_models_text():
    models_list = ["ü§ñ **–î–æ—Å—Ç—É–ø–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ (–ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É):**\n"]
    sorted_models = sorted(AVAILABLE_MODELS.items(), key=lambda x: x[1].get("priority", 99))
    for key, model in sorted_models:
        models_list.append(f"‚Ä¢ {model['display_name']}")
    return "\n".join(models_list)

def clean_response(text):
    if not text: return ""
    text = str(text)
    # –ß–∏—Å—Ç–∫–∞ —Ç–µ–≥–æ–≤ –º—ã—à–ª–µ–Ω–∏—è
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    text = re.sub(r'^(Bot|System|Assistant|Yachejka|–Ø—á–µ–π–∫–∞):\s*', '', text.strip(), flags=re.IGNORECASE)
    return text.strip()

def is_refusal(text):
    text_lower = text.lower()
    triggers = ["language model", "–Ω–µ –º–æ–≥—É", "–Ω–µ—ç—Ç–∏—á–Ω–æ", "ai assistant", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"]
    return len(text) < 200 and any(t in text_lower for t in triggers)

def is_summary_query(text):
    triggers = ["—Å–∞–º–º–∞—Ä–∏", "summary", "—Å–≤–æ–¥–∫–∞", "–∏—Ç–æ–≥–∏", "–ø–µ—Ä–µ—Å–∫–∞–∂–∏", "–∫—Ä–∞—Ç–∫–æ", "–æ —á–µ–º —Ä–µ—á—å"]
    return text and any(t in text.lower() for t in triggers)

def is_event_query(text):
    triggers = ["–∫—É–¥–∞ —Å—Ö–æ–¥–∏—Ç—å", "–∞–Ω–æ–Ω—Å", "–≤—Å—Ç—Ä–µ—á–∞", "–ø–ª–∞–Ω—ã", "–∏–≤–µ–Ω—Ç", "—Å—Ö–æ–¥–∫–∞"]
    return text and any(t in text.lower() for t in triggers)

def get_system_prompt(memory_text="", query_type="chat"):
    prompt = BASE_SYSTEM_PROMPT
    
    if memory_text:
        prompt += f"\n[–§–ê–ö–¢–´ –û –°–û–ë–ï–°–ï–î–ù–ò–ö–ï]: {memory_text}"
        
    if query_type == "summary":
        prompt += "\n–ó–ê–î–ê–ß–ê: –°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É –ø–µ—Ä–µ–ø–∏—Å–∫–∏. –ö—Ç–æ —á—Ç–æ –ø–∏—Å–∞–ª, –æ —á–µ–º —Å–ø–æ—Ä–∏–ª–∏. –ë–µ–∑ –≤–æ–¥—ã."
    elif query_type == "events":
        prompt += "\n–ó–ê–î–ê–ß–ê: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ø–∏—Å–æ–∫ –∞–Ω–æ–Ω—Å–æ–≤ –∏ –ø–æ–¥—Å–∫–∞–∂–∏, –∫—É–¥–∞ —Å—Ç–æ–∏—Ç —Å—Ö–æ–¥–∏—Ç—å."
    else:
        prompt += "\n–í–ê–ñ–ù–û: –ü–æ–º–Ω–∏ –ø—Ä–æ —Å–≤–æ–π –ñ–ï–ù–°–ö–ò–ô –ø–æ–ª (–¥–µ–ª–∞–ª–∞, –≤–∏–¥–µ–ª–∞). –û—Ç–≤–µ—á–∞–π –∫–æ—Ä–æ—Ç–∫–æ –∏ –∂–∏–≤–æ."
        
    return prompt

async def generate_response(db, chat_id, thread_id, current_message, bot, image_data=None, user_id=None):
    # 1. –°–±–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    limit_history = 50 if is_summary_query(current_message) else 8
    history_rows = await db.get_context(chat_id, thread_id, limit=limit_history)
    
    # 2. –ü–∞–º—è—Ç—å
    memory_text = ""
    if user_id:
        facts = await db.get_relevant_facts(chat_id, user_id)
        if facts:
            lines = [f"- {f['fact']}" for f in facts[:2]]
            memory_text = "; ".join(lines)

    # 3. –ê–Ω–æ–Ω—Å—ã (–µ—Å–ª–∏ –Ω—É–∂–Ω—ã)
    found_events_text = ""
    query_type = "chat"
    
    if is_summary_query(current_message):
        query_type = "summary"
    elif is_event_query(current_message):
        query_type = "events"
        raw_events = await db.get_potential_announcements(chat_id, days=30, limit=3)
        if raw_events:
            lines = [f"- {e.get('content')[:150]}..." for e in raw_events]
            found_events_text = "\n".join(lines)

    # 4. –°–±–æ—Ä–∫–∞ –ø—Ä–æ–º–ø—Ç–∞
    system_prompt = get_system_prompt(memory_text, query_type)
    
    if query_type == "events" and found_events_text:
        system_prompt += f"\n\n[–ù–ê–ô–î–ï–ù–ù–´–ï –ê–ù–û–ù–°–´]:\n{found_events_text}"
    elif query_type == "events":
        system_prompt += "\n\n[–ê–ù–û–ù–°–´]: –ù–µ –Ω–∞–π–¥–µ–Ω–æ. –°–∫–∞–∂–∏, —á—Ç–æ –ø–æ–∫–∞ –≥–ª—É—Ö–æ."

    messages = [{"role": "system", "content": system_prompt}]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
    for row in history_rows:
        role = "assistant" if row['role'] == "model" else "user"
        content = clean_response(row.get('content'))
        name = row.get('user_name', 'User')
        if content:
            msg = f"{name}: {content}" if role == "user" else content
            messages.append({"role": role, "content": msg})

    # –¢–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    user_content = [{"type": "text", "text": current_message}]
    if image_data:
        try:
            buffered = io.BytesIO()
            image_data.save(buffered, format="JPEG", quality=80)
            b64 = base64.b64encode(buffered.getvalue()).decode("utf-8")
            user_content.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}})
        except: pass

    messages.append({"role": "user", "content": user_content})

    # –í—ã–±–æ—Ä –æ—á–µ—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π
    if image_data:
        queue = sorted([m for m in AVAILABLE_MODELS.values() if m["multimodal"]], key=lambda x: x["priority"])
    else:
        # –î–ª—è —Ç–µ–∫—Å—Ç–∞ –±–µ—Ä–µ–º –ª—é–±—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é
        queue = sorted(AVAILABLE_MODELS.values(), key=lambda x: x["priority"])

    # –ó–∞–ø—Ä–æ—Å –∫ API
    for model_cfg in queue:
        try:
            response = await client.chat.completions.create(
                model=model_cfg["name"],
                messages=messages,
                temperature=0.7, # –ù–µ–º–Ω–æ–≥–æ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
                max_tokens=1000,
            )
            reply = clean_response(response.choices[0].message.content)
            
            if not reply or is_refusal(reply):
                continue
                
            return reply
        except Exception as e:
            logging.error(f"Model {model_cfg['name']} failed: {e}")
            continue

    return "–ß—Ç–æ-—Ç–æ —è –ø—Ä–∏—É–Ω—ã–ª–∞... (–æ—à–∏–±–∫–∞ API)"
